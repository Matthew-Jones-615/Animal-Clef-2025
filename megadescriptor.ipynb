{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":91451,"databundleVersionId":11223220,"sourceType":"competition"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport pandas as pd\nfrom torchvision.models.vision_transformer import vit_b_16, ViT_B_16_Weights\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\n# =============================\n# Configurations\n# =============================\nBATCH_SIZE = 32\nNUM_WORKERS = 4\nIMAGE_SIZE = 224\nEPOCHS_CLP = 40\nEPOCHS_PLF = 40\nLR = 1e-4\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nCHECKPOINT_DIR = './checkpoints'\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\n\n# reproducibility\ntorch.manual_seed(42)\n\n# =============================\n# Dataset\n# =============================\nclass AnimalCLEFDataset(Dataset):\n    def __init__(self, root, split=\"database\", transform=None):\n        self.root = root.rstrip('/')\n        meta = pd.read_csv(f\"{self.root}/metadata.csv\")\n        sel = meta[meta['path'].str.contains(f\"/{split}/\")].reset_index(drop=True)\n        if sel.empty:\n            raise ValueError(f\"No entries for split '{split}'\")\n\n        self.paths = sel['path'].tolist()\n        self.image_ids = sel['image_id'].tolist()\n\n        if split == 'database':\n            #  Use individual identity,  \n            ids = sel['identity'].astype(str)\n\n            #  Build mapping from identity string → label index\n            self.id2idx = {iid: i for i, iid in enumerate(sorted(ids.unique()))}\n\n            #  Map each sample's identity to its label\n            self.labels = ids.map(self.id2idx).tolist()\n\n            # Safety check\n            num_classes = len(self.id2idx)\n            assert all(0 <= label < num_classes for label in self.labels), \"Invalid labels found\"\n        else:\n            self.labels = [-1] * len(sel)\n\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.paths)\n\n    def __getitem__(self, i):\n        img = Image.open(f\"{self.root}/{self.paths[i]}\").convert('RGB')\n        if self.transform:\n            img = self.transform(img)\n        return img, self.labels[i]\n\n# =============================\n# MAE Encoder + Projection Head + Decoder\n# =============================\nclass MAEFramework(nn.Module):\n    def __init__(self,\n                 embed_dim: int = 768,\n                 proj_dim: int = 256,\n                 decoder_dim: int = 256,\n                 layer_indices: list[int] = [3, 6, 9]):\n        super().__init__()\n        self.embed_dim = embed_dim\n        # 1) Backbone ViT\n        self.encoder = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n        self.encoder.head = nn.Identity()\n        self.layer_indices = set(layer_indices)\n\n        # 3) Projection head\n        self.proj_head = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(embed_dim, proj_dim),\n        )\n        # 4) Decoder\n        self.decoder = nn.Sequential(\n            nn.Linear(embed_dim, decoder_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(decoder_dim, 3 * IMAGE_SIZE * IMAGE_SIZE),\n        )\n\n    def forward(self, x: torch.Tensor, return_feats: bool = False):\n        B = x.size(0)\n        # patch embed\n        x_p = self.encoder.conv_proj(x)\n        x_p = x_p.flatten(2).transpose(1, 2)\n        cls_tok = self.encoder.class_token.expand(B, -1, -1)\n        tokens = torch.cat([cls_tok, x_p], dim=1)\n        tokens = tokens + self.encoder.encoder.pos_embedding\n\n        feats = []\n        for idx, block in enumerate(self.encoder.encoder.layers):\n            tokens = block(tokens)\n            if idx in self.layer_indices:\n                feats.append(tokens.clone())\n\n        cls_feat = self.encoder.encoder.ln(tokens[:, 0])\n        proj = self.proj_head(cls_feat)\n        rec = self.decoder(cls_feat).view(B, 3, IMAGE_SIZE, IMAGE_SIZE)\n        rec_loss = F.mse_loss(rec, x, reduction='none').mean([1, 2, 3])\n\n        if return_feats:\n            return cls_feat, proj, rec_loss, rec, feats\n        return proj, rec_loss, rec\n\n# =============================\n# Losses\n# =============================\nclass SupConLoss(nn.Module):\n    def __init__(self, temperature=0.5):\n        super().__init__()\n        self.temp = temperature\n\n    def forward(self, p1, p2, labels=None):\n        # NT-Xent instance-level contrastive loss\n        z = torch.cat([p1, p2], dim=0)\n        z = F.normalize(z, dim=1)\n        N = p1.size(0)\n        sim = torch.matmul(z, z.T) / self.temp\n        mask = torch.eye(2*N, device=sim.device).bool()\n        sim.masked_fill_(mask, -9e15)\n        idx = torch.arange(N, device=sim.device)\n        targets = torch.cat([idx + N, idx])\n        return F.cross_entropy(sim, targets)\n\nclass ProtoLoss(nn.Module):\n    def forward(self, feats, prototypes, labels):\n        dist = torch.cdist(feats, prototypes)\n        return F.cross_entropy(-dist, labels)\n\n# =============================\n# Utilities\n# =============================\ndef compute_layer_distances(bef_feats, aft_feats, temperature=0.5):\n    total = []\n    for b, a in zip(bef_feats, aft_feats):  # each b/a: (B, N, D)\n        b_f, a_f = b.flatten(1), a.flatten(1)  # (B, N*D)\n        eu = F.pairwise_distance(b_f, a_f)  # (B,)\n        cos = 1 - F.cosine_similarity(b_f, a_f, dim=1)  # (B,)\n        score = eu + temperature * cos  # (B,)\n        total.append(score)  # keep per-sample scores\n    return torch.stack(total, dim=0).mean(dim=0)  # (B,)\n\n\ndef calculate_unknown_score(feat_bef, feat_aft, feat_vec, prototypes, lamda=1.0):\n    s_total = compute_layer_distances(feat_bef, feat_aft)  # shape: (B,)\n    \n    fv_n = F.normalize(feat_vec, dim=1)\n    p_n = F.normalize(prototypes, dim=1)\n    sim = torch.matmul(fv_n, p_n.T)  # (B, C)\n    s_proto, _ = sim.max(dim=1)  # max similarity per sample\n\n    unknown_score = (1 - s_proto) + lamda * s_total\n    return unknown_score  # higher = more unknown\n\n\n\n\n# =============================\n# Training: CLP\n# =============================\ndef train_CLP(model, loader, epochs, lr, ckpt, alpha_cl=1.0, alpha_rec=1.0):\n    path = os.path.join(CHECKPOINT_DIR, ckpt)\n    opt = torch.optim.Adam(model.parameters(), lr=lr)\n    start = 0\n    if os.path.exists(path):\n        ck = torch.load(path)\n        model.load_state_dict(ck['model'])\n        opt.load_state_dict(ck['opt'])\n        start = ck['ep'] + 1\n    scl = SupConLoss()\n    model.to(DEVICE)\n\n    for ep in range(start, epochs):\n        model.train()\n        total = 0.0\n        for x, y in tqdm(loader, desc=f\"CLP Ep{ep+1}\"):\n            x, y = x.to(DEVICE), y.to(DEVICE)\n            x1 = x + 0.05 * torch.randn_like(x)\n            x2 = x + 0.05 * torch.randn_like(x)\n            f1, p1, r1_loss, r1, _ = model(x1, return_feats=True)\n            f2, p2, r2_loss, r2, _ = model(x2, return_feats=True)\n            l_cl = scl(p1, p2)\n            l_rec = r1_loss.mean() + r2_loss.mean()\n            loss = alpha_cl * l_cl + alpha_rec * l_rec\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n            total += loss.item()\n        torch.save({'model': model.state_dict(), 'opt': opt.state_dict(), 'ep': ep}, path)\n        print(f\"CLP Epoch {ep+1}: {total/len(loader):.4f}\")\n\n\n# =============================\n# Training: PLF\n# =============================\ndef train_PLF(model, loader, epochs, lr, ckpt, num_classes, encoder_pre):\n    path = os.path.join(CHECKPOINT_DIR, ckpt)\n    # unwrap if DataParallel\n    base_model = model.module if isinstance(model, nn.DataParallel) else model\n    embed_dim = base_model.embed_dim\n    proto_tensor = torch.randn(num_classes, embed_dim, device=DEVICE)\n    proto = nn.Parameter(proto_tensor, requires_grad=True)\n    opt = torch.optim.Adam(list(model.parameters()) + [proto], lr=lr)\n    start = 0\n    if os.path.exists(path):\n        ck = torch.load(path)\n        model.load_state_dict(ck['model'])\n        proto.data = ck['proto']\n        opt.load_state_dict(ck['opt'])\n        start = ck['ep'] + 1\n    ploss = ProtoLoss()\n    model.to(DEVICE)\n\n    for ep in range(start, epochs):\n        model.train()\n        total = 0.0\n        for x, y in tqdm(loader, desc=f\"PLF Ep{ep+1}\"):\n            x, y = x.to(DEVICE), y.to(DEVICE)\n            feats, _, _, _, _ = model(x, return_feats=True)\n            loss = ploss(feats, proto, y)\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n            total += loss.item()\n        torch.save({'model': model.state_dict(), 'proto': proto.data, 'opt': opt.state_dict(), 'ep': ep}, path)\n        print(f\"PLF Epoch {ep+1}: {total/len(loader):.4f}\")\n    return proto\n\n# =============================\n# Inference\n# =============================\n\n@torch.no_grad()\ndef inference(enc_pre, model, proto, loader, threshold, lamda=0.2):\n    enc_pre.eval()\n    model.eval()\n    preds = []\n\n    for x, _ in tqdm(loader, desc=\"Infer\"):\n        x = x.to(DEVICE)\n        enc_pre = enc_pre.to(DEVICE)\n        model = model.to(DEVICE)\n\n        _, _, _, _, bef_feats = enc_pre(x, return_feats=True)\n        feat, _, _, _, aft_feats = model(x, return_feats=True)\n\n        # 1. Calculate unknown score\n        scores = calculate_unknown_score(bef_feats, aft_feats, feat, proto, lamda=lamda)  # shape: (B,)\n        print(\"Score stats:\", torch.min(scores).item(), torch.mean(scores).item(), torch.max(scores).item())\n\n        # 2. Predict the most similar known class\n        idx = torch.argmax(torch.matmul(F.normalize(feat, dim=1), F.normalize(proto, dim=1).T), dim=1)  # (B,)\n\n        # 3. Threshold to filter unknowns\n        known = scores < threshold\n        pred = idx.clone()\n        pred[~known] = -1  #  invert known to get unknown\n\n\n        preds.append(pred.cpu())\n\n    return torch.cat(preds, dim=0)\n\n\n# =============================\n# Submission\n# =============================\ndef generate_submission(root, preds, db_ds):\n    sub = pd.read_csv(f\"{root}/sample_submission.csv\")\n    meta = pd.read_csv(f\"{root}/metadata.csv\")\n    q = meta[meta['path'].str.contains('/query/')].reset_index(drop=True)\n    q['pred_idx'] = preds.numpy()\n\n    # Step 1: Print raw predicted indices\n    print(\"\\nRaw predicted indices (pred_idx):\")\n    print(q[['image_id', 'pred_idx']].head(10))\n\n    # Step 2: Index to identity mapping\n    idx2id = {v: k for k, v in db_ds.id2idx.items()}\n    q['prediction'] = q['pred_idx'].apply(lambda i: 'new_individual' if i < 0 else idx2id.get(int(i), f\"unknown_{i}\"))\n\n    # Step 3: Print mapped predictions\n    print(\"\\nMapped predictions (after idx2id):\")\n    print(q[['image_id', 'prediction']].head(10))\n\n    out = sub[['image_id']].merge(q[['image_id', 'prediction']], on='image_id')\n\n    import time\n    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n    save_path = f'/kaggle/working/submission_{timestamp}.csv'\n    out.to_csv(save_path, index=False)\n\n    return save_path\n \n# =============================\n# Main Workflow\n# =============================\ndef main():\n    root = '/kaggle/input/animal-clef-2025'\n    tf = transforms.Compose([\n        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n        transforms.ToTensor()\n    ])\n    #idx2id = {v: k for k, v in db_ds.id2idx.items()}\n\n    db_ds = AnimalCLEFDataset(root, 'database', transform=tf)\n    db_loader = DataLoader(db_ds, batch_size=8, shuffle=True,\n                           num_workers=NUM_WORKERS, pin_memory=True)\n    query_ds = AnimalCLEFDataset(root, 'query', transform=tf)\n    q_loader = DataLoader(query_ds, batch_size=8, shuffle=False,\n                           num_workers=NUM_WORKERS, pin_memory=True)\n\n    clp_model = MAEFramework()\n    plf_model = MAEFramework()\n\n    num_gpus = torch.cuda.device_count()\n    if num_gpus >= 2:\n        device_ids = [0, 1]\n    elif num_gpus == 1:\n        device_ids = [0]\n    else:\n        device_ids = None\n\n    clp_model = nn.DataParallel(clp_model, device_ids=device_ids).to(DEVICE)\n    plf_model = nn.DataParallel(plf_model, device_ids=device_ids).to(DEVICE)\n\n    train_CLP(clp_model, db_loader, EPOCHS_CLP, LR, 'clp.pth', alpha_cl=1.0, alpha_rec=1.0)\n    prototype = train_PLF(plf_model, db_loader, EPOCHS_PLF, LR,\n                          'plf.pth', len(db_ds.id2idx), clp_model)\n    print(\"Prototype shape:\", prototype.shape)\n    print(\"Number of unique classes:\", len(db_ds.id2idx))\n\n    # Plot the score distribution before estimating threshold\n    #plot_score_distribution(clp_model, plf_model, prototype, db_loader)\n\n    # Estimate threshold from database distribution\n    dists = []\n    with torch.no_grad():\n        for x, _ in DataLoader(db_ds, batch_size=8, shuffle=False,\n                               num_workers=NUM_WORKERS, pin_memory=True):\n            x = x.to(DEVICE)\n            feat, _, _, _, aft_feats = plf_model(x, return_feats=True)\n            _, _, _, _, bef_feats = clp_model(x, return_feats=True)\n            scores = calculate_unknown_score(bef_feats, aft_feats, feat, prototype, lamda = 0.2)\n            dists.extend(scores.cpu().tolist())\n\n\n    # Ensure flat list of floats\n    flat_dists = [v.item() if isinstance(v, torch.Tensor) else v for v in dists]\n    threshold = torch.quantile(torch.tensor(flat_dists, dtype=torch.float32), 0.95)\n\n\n    print(\"Estimated threshold:\", threshold.item())\n    print(\"Score range — min:\", min(flat_dists), \"max:\", max(flat_dists))\n\n\n    # Inference on query set\n    with torch.no_grad():\n        lamda = 0.2 # or any value you want to test\n        preds = inference(clp_model, plf_model, prototype, q_loader, threshold, lamda=lamda)\n    print(\"Total predictions:\", len(preds))\n    print(\"Predicted as new_individual:\", (preds == -1).sum().item())\n\n    print(\"Saving submission…\")\n    generate_submission(root, preds, db_ds)\n\nif __name__ == '__main__':\n    main()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T21:01:09.470837Z","iopub.execute_input":"2025-05-09T21:01:09.471471Z","execution_failed":"2025-05-10T01:48:00.121Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/1936975804.py:184: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  ck = torch.load(path)\nCLP Ep21: 100%|██████████| 1635/1635 [14:03<00:00,  1.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"CLP Epoch 21: 1.2769\n","output_type":"stream"},{"name":"stderr","text":"CLP Ep22: 100%|██████████| 1635/1635 [14:03<00:00,  1.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"CLP Epoch 22: 1.1955\n","output_type":"stream"},{"name":"stderr","text":"CLP Ep23: 100%|██████████| 1635/1635 [14:04<00:00,  1.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"CLP Epoch 23: 1.1957\n","output_type":"stream"},{"name":"stderr","text":"CLP Ep24: 100%|██████████| 1635/1635 [14:04<00:00,  1.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"CLP Epoch 24: 1.1872\n","output_type":"stream"},{"name":"stderr","text":"CLP Ep25:  67%|██████▋   | 1088/1635 [09:22<04:42,  1.93it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"20% Accuracy","metadata":{}},{"cell_type":"code","source":"# ====== Imports and Setup ======\nimport os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport timm\nimport joblib\n\n# ====== Device ======\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ====== Configuration ======\nroot_dir = \"/kaggle/input/animal-clef-2025\"\nmetadata_path = \"/kaggle/input/animal-clef-2025/metadata.csv\"\nembedding_dim = 512\nconfidence_threshold = 0.90\n\n# ====== Load Metadata ======\ndf = pd.read_csv(metadata_path)\n\n# ====== Encoder ======\nencoder = LabelEncoder()\ndatabase_df = df[df[\"split\"] == \"database\"].dropna(subset=[\"identity\"])\ndatabase_df[\"label\"] = encoder.fit_transform(database_df[\"identity\"])\njoblib.dump(encoder, \"label_encoder.pkl\")\n\n# ====== Transform ======\ntransform = transforms.Compose([\n    transforms.Resize((384, 384)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3),\n])\n\n# ====== Dataset Class ======\nclass InferenceDataset(Dataset):\n    def __init__(self, df, transform):\n        self.paths = df[\"path\"].tolist()\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.paths)\n\n    def __getitem__(self, idx):\n        img = Image.open(os.path.join(root_dir, self.paths[idx])).convert(\"RGB\")\n        img = self.transform(img)\n        return img\n\n# ====== Load Model ======\nmodel = timm.create_model(\"hf-hub:BVRA/MegaDescriptor-L-384\", pretrained=True)\nmodel = model.to(device)\nmodel.eval()\n\n# ====== Embedding Function ======\ndef extract_embeddings(model, df, transform, batch_size=64):\n    loader = DataLoader(InferenceDataset(df, transform), batch_size=batch_size, shuffle=False, num_workers=2)\n    embeddings = []\n    with torch.no_grad():\n        for batch in tqdm(loader, desc=\"Extracting embeddings\"):\n            batch = batch.to(device)\n            emb = model(batch)\n            embeddings.append(emb.cpu().numpy())\n    return np.vstack(embeddings)\n\n# ====== Prepare and Embed ======\nall_results = []\nfor class_name in [\"SeaTurtleID2022\", \"LynxID2025\", \"SalamanderID2025\"]:\n    class_df = df[df[\"path\"].str.contains(class_name)]\n    database_df = class_df[class_df[\"split\"] == \"database\"].dropna(subset=[\"identity\"])\n    query_df = class_df[class_df[\"split\"] == \"query\"]\n\n    db_embeddings = extract_embeddings(model, database_df, transform)\n    query_embeddings = extract_embeddings(model, query_df, transform)\n\n    db_labels = database_df[\"identity\"].tolist()\n    for i, query_emb in enumerate(query_embeddings):\n        sims = cosine_similarity(query_emb.reshape(1, -1), db_embeddings)[0]\n        max_idx = np.argmax(sims)\n        max_sim = sims[max_idx]\n        identity = db_labels[max_idx] if max_sim >= confidence_threshold else \"new_individual\"\n        all_results.append({\n            \"image_id\": query_df.iloc[i][\"image_id\"],\n            \"identity\": identity\n        })\n\n# ====== Save Submission ======\nsubmission_df = pd.DataFrame(all_results)\nsubmission_df.to_csv(\"submission.csv\", index=False)\nprint(\"Submission saved as submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T22:22:24.149134Z","iopub.execute_input":"2025-05-10T22:22:24.149714Z","iopub.status.idle":"2025-05-10T22:34:30.063995Z","shell.execute_reply.started":"2025-05-10T22:22:24.149681Z","shell.execute_reply":"2025-05-10T22:34:30.062981Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b282f181382d4d6d9db69eb1db9a4182"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e98ce7ba27cf4e04ab766c7dd0cf1af2"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 137/137 [06:15<00:00,  2.74s/it]\nExtracting embeddings: 100%|██████████| 8/8 [00:22<00:00,  2.85s/it]\nExtracting embeddings: 100%|██████████| 47/47 [02:09<00:00,  2.75s/it]\nExtracting embeddings: 100%|██████████| 15/15 [00:43<00:00,  2.91s/it]\nExtracting embeddings: 100%|██████████| 22/22 [01:02<00:00,  2.84s/it]\nExtracting embeddings: 100%|██████████| 11/11 [00:32<00:00,  2.96s/it]\n","output_type":"stream"},{"name":"stdout","text":"Submission saved as submission.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"27% Accuracy","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport timm\nimport joblib\n\n# ====== Configuration ======\nclass Config:\n    ROOT_DIR = \"/kaggle/input/animal-clef-2025\"\n    METADATA_PATH = os.path.join(ROOT_DIR, \"metadata.csv\")\n    EMBEDDING_DIM = 512\n    BATCH_SIZE = 32\n    NUM_WORKERS = 2\n    IMAGE_SIZE = 384\n    CLASSES = [\"SeaTurtleID2022\", \"LynxID2025\", \"SalamanderID2025\"]\n    MODEL_NAME = \"hf-hub:BVRA/MegaDescriptor-L-384\"\n    BASE_THRESHOLD = 0.85\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ====== Dataset and Model Setup ======\nclass AnimalDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df.reset_index(drop=True)\n        self.transform = transform or transforms.Compose([\n            transforms.Resize((Config.IMAGE_SIZE, Config.IMAGE_SIZE)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n        ])\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        img_path = os.path.join(Config.ROOT_DIR, self.df.iloc[idx]['path'])\n        try:\n            with Image.open(img_path) as img:\n                return self.transform(img.convert(\"RGB\"))\n        except:\n            return torch.zeros((3, Config.IMAGE_SIZE, Config.IMAGE_SIZE))\n\nclass FeatureExtractor:\n    def __init__(self):\n        self.model = timm.create_model(Config.MODEL_NAME, pretrained=True)\n        self.model = self.model.to(device).eval()\n        \n    @torch.no_grad()\n    def extract_embeddings(self, df):\n        dataset = AnimalDataset(df)\n        loader = DataLoader(dataset, batch_size=Config.BATCH_SIZE,\n                          num_workers=Config.NUM_WORKERS)\n        \n        embeddings = []\n        for batch in tqdm(loader, desc=\"Extracting embeddings\"):\n            embeddings.append(self.model(batch.to(device)).cpu())\n        return torch.cat(embeddings).numpy()\n\n# ====== Similarity Search ======\ndef find_matches(query_emb, db_embeddings, db_df, threshold):\n    \"\"\"NumPy implementation of similarity search\"\"\"\n    sims = cosine_similarity(query_emb.reshape(1, -1), db_embeddings)[0]\n    max_idx = np.argmax(sims)\n    return db_df.iloc[max_idx][\"identity\"] if sims[max_idx] >= threshold else \"new_individual\"\n\n# ====== Main Execution ======\nif __name__ == \"__main__\":\n    df = pd.read_csv(Config.METADATA_PATH)\n    extractor = FeatureExtractor()\n    results = []\n    \n    for class_name in Config.CLASSES:\n        print(f\"\\nProcessing {class_name}...\")\n        class_df = df[df[\"path\"].str.contains(class_name)]\n        db_df = class_df[class_df[\"split\"] == \"database\"].dropna(subset=[\"identity\"])\n        query_df = class_df[class_df[\"split\"] == \"query\"]\n        \n        if db_df.empty or query_df.empty:\n            continue\n            \n        db_embeddings = extractor.extract_embeddings(db_df)\n        query_embeddings = extractor.extract_embeddings(query_df)\n        \n        # Normalize embeddings\n        db_embeddings = db_embeddings / np.linalg.norm(db_embeddings, axis=1, keepdims=True)\n        query_embeddings = query_embeddings / np.linalg.norm(query_embeddings, axis=1, keepdims=True)\n        \n        for i, query_emb in enumerate(tqdm(query_embeddings, desc=\"Matching\")):\n            identity = find_matches(query_emb, db_embeddings, db_df, Config.BASE_THRESHOLD)\n            results.append({\n                \"image_id\": query_df.iloc[i][\"image_id\"],\n                \"identity\": identity\n            })\n    \n    pd.DataFrame(results).to_csv(\"submission.csv\", index=False)\n    print(\"Done! Results saved to submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T22:36:29.991187Z","iopub.execute_input":"2025-05-10T22:36:29.991879Z","iopub.status.idle":"2025-05-10T22:48:05.268972Z","shell.execute_reply.started":"2025-05-10T22:36:29.991853Z","shell.execute_reply":"2025-05-10T22:48:05.268077Z"}},"outputs":[{"name":"stdout","text":"\nProcessing SeaTurtleID2022...\n","output_type":"stream"},{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 273/273 [06:12<00:00,  1.37s/it]\nExtracting embeddings: 100%|██████████| 16/16 [00:21<00:00,  1.37s/it]\nMatching: 100%|██████████| 500/500 [00:19<00:00, 26.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nProcessing LynxID2025...\n","output_type":"stream"},{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 93/93 [02:07<00:00,  1.37s/it]\nExtracting embeddings: 100%|██████████| 30/30 [00:41<00:00,  1.39s/it]\nMatching: 100%|██████████| 946/946 [00:12<00:00, 76.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nProcessing SalamanderID2025...\n","output_type":"stream"},{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 44/44 [01:00<00:00,  1.37s/it]\nExtracting embeddings: 100%|██████████| 22/22 [00:30<00:00,  1.39s/it]\nMatching: 100%|██████████| 689/689 [00:03<00:00, 185.11it/s]","output_type":"stream"},{"name":"stdout","text":"Done! Results saved to submission.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"22% with argumentations","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport timm\nimport joblib\n\n# ====== Configuration ======\nclass Config:\n    ROOT_DIR = \"/kaggle/input/animal-clef-2025\"\n    METADATA_PATH = os.path.join(ROOT_DIR, \"metadata.csv\")\n    EMBEDDING_DIM = 512\n    BATCH_SIZE = 32\n    NUM_WORKERS = 2\n    IMAGE_SIZE = 384\n    CLASSES = [\"SeaTurtleID2022\", \"LynxID2025\", \"SalamanderID2025\"]\n    MODEL_NAME = \"hf-hub:BVRA/MegaDescriptor-L-384\"\n    BASE_THRESHOLD = 0.85\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ====== Dataset and Model Setup ======\nclass AnimalDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df.reset_index(drop=True)\n        self.transform = transform or transforms.Compose([\n                transforms.Resize((384, 384)),  # Keep consistent with model\n                transforms.RandomAffine(degrees=15, translate=(0.1, 0.1)),  # Add slight augmentation\n                transforms.ColorJitter(brightness=0.1, contrast=0.1),  # Help with lighting variations\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n        ])\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        img_path = os.path.join(Config.ROOT_DIR, self.df.iloc[idx]['path'])\n        try:\n            with Image.open(img_path) as img:\n                return self.transform(img.convert(\"RGB\"))\n        except:\n            return torch.zeros((3, Config.IMAGE_SIZE, Config.IMAGE_SIZE))\n\nclass FeatureExtractor:\n    def __init__(self):\n        self.model = timm.create_model(Config.MODEL_NAME, pretrained=True)\n        self.model = self.model.to(device).eval()\n        \n    @torch.no_grad()\n    def extract_embeddings(self, df):\n        dataset = AnimalDataset(df)\n        loader = DataLoader(dataset, batch_size=Config.BATCH_SIZE,\n                          num_workers=Config.NUM_WORKERS)\n        \n        embeddings = []\n        for batch in tqdm(loader, desc=\"Extracting embeddings\"):\n            embeddings.append(self.model(batch.to(device)).cpu())\n        return torch.cat(embeddings).numpy()\n\n# ====== Similarity Search ======\ndef find_matches(query_emb, db_embeddings, db_df, threshold):\n    \"\"\"NumPy implementation of similarity search\"\"\"\n    sims = cosine_similarity(query_emb.reshape(1, -1), db_embeddings)[0]\n    max_idx = np.argmax(sims)\n    return db_df.iloc[max_idx][\"identity\"] if sims[max_idx] >= threshold else \"new_individual\"\n\n# ====== Main Execution ======\nif __name__ == \"__main__\":\n    df = pd.read_csv(Config.METADATA_PATH)\n    extractor = FeatureExtractor()\n    results = []\n    \n    for class_name in Config.CLASSES:\n        print(f\"\\nProcessing {class_name}...\")\n        class_df = df[df[\"path\"].str.contains(class_name)]\n        db_df = class_df[class_df[\"split\"] == \"database\"].dropna(subset=[\"identity\"])\n        query_df = class_df[class_df[\"split\"] == \"query\"]\n        \n        if db_df.empty or query_df.empty:\n            continue\n            \n        db_embeddings = extractor.extract_embeddings(db_df)\n        query_embeddings = extractor.extract_embeddings(query_df)\n        \n        # Normalize embeddings\n        db_embeddings = db_embeddings / np.linalg.norm(db_embeddings, axis=1, keepdims=True)\n        query_embeddings = query_embeddings / np.linalg.norm(query_embeddings, axis=1, keepdims=True)\n        \n        for i, query_emb in enumerate(tqdm(query_embeddings, desc=\"Matching\")):\n            identity = find_matches(query_emb, db_embeddings, db_df, Config.BASE_THRESHOLD)\n            results.append({\n                \"image_id\": query_df.iloc[i][\"image_id\"],\n                \"identity\": identity\n            })\n    \n    pd.DataFrame(results).to_csv(\"submission.csv\", index=False)\n    print(\"Done! Results saved to submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T22:54:29.422183Z","iopub.execute_input":"2025-05-10T22:54:29.422520Z","iopub.status.idle":"2025-05-10T23:06:04.771324Z","shell.execute_reply.started":"2025-05-10T22:54:29.422497Z","shell.execute_reply":"2025-05-10T23:06:04.770523Z"}},"outputs":[{"name":"stdout","text":"\nProcessing SeaTurtleID2022...\n","output_type":"stream"},{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 273/273 [06:12<00:00,  1.36s/it]\nExtracting embeddings: 100%|██████████| 16/16 [00:22<00:00,  1.38s/it]\nMatching: 100%|██████████| 500/500 [00:18<00:00, 26.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nProcessing LynxID2025...\n","output_type":"stream"},{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 93/93 [02:07<00:00,  1.37s/it]\nExtracting embeddings: 100%|██████████| 30/30 [00:41<00:00,  1.40s/it]\nMatching: 100%|██████████| 946/946 [00:12<00:00, 73.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nProcessing SalamanderID2025...\n","output_type":"stream"},{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 44/44 [01:00<00:00,  1.38s/it]\nExtracting embeddings: 100%|██████████| 22/22 [00:30<00:00,  1.40s/it]\nMatching: 100%|██████████| 689/689 [00:03<00:00, 190.61it/s]","output_type":"stream"},{"name":"stdout","text":"Done! Results saved to submission.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.model_selection import train_test_split\nimport timm\nimport joblib\nfrom collections import defaultdict\n\n# ====== Configuration ======\nclass Config:\n    ROOT_DIR = \"/kaggle/input/animal-clef-2025\"\n    METADATA_PATH = os.path.join(ROOT_DIR, \"metadata.csv\")\n    EMBEDDING_DIM = 512\n    BATCH_SIZE = 64  # Increased batch size\n    NUM_WORKERS = 4\n    IMAGE_SIZE = 384\n    CLASSES = [\"SeaTurtleID2022\", \"LynxID2025\", \"SalamanderID2025\"]\n    MODEL_NAME = \"hf-hub:BVRA/MegaDescriptor-L-384\"\n    BASE_THRESHOLD = 0.87  # Adjusted threshold\n    VAL_SIZE = 0.2\n    RANDOM_SEED = 42\n    MIN_SAMPLES_PER_CLASS = 2  # Minimum samples required per class\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ====== Dataset Class with Enhanced Augmentation ======\nclass AnimalDataset(Dataset):\n    def __init__(self, df, transform=None, is_train=True):\n        self.df = df.reset_index(drop=True)\n        self.is_train = is_train\n        self.transform = transform or self.get_default_transform(is_train)\n        \n    def get_default_transform(self, is_train):\n        if is_train:\n            return transforms.Compose([\n                transforms.Resize((Config.IMAGE_SIZE, Config.IMAGE_SIZE)),\n                transforms.RandomHorizontalFlip(),\n                transforms.RandomAffine(degrees=10, translate=(0.1, 0.1)),\n                transforms.ColorJitter(brightness=0.2, contrast=0.2),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n            ])\n        else:\n            return transforms.Compose([\n                transforms.Resize((Config.IMAGE_SIZE, Config.IMAGE_SIZE)),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n            ])\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        img_path = os.path.join(Config.ROOT_DIR, self.df.iloc[idx]['path'])\n        try:\n            with Image.open(img_path) as img:\n                if self.transform:\n                    img = self.transform(img.convert(\"RGB\"))\n                return img\n        except Exception as e:\n            print(f\"Error loading {img_path}: {str(e)}\")\n            return torch.zeros((3, Config.IMAGE_SIZE, Config.IMAGE_SIZE))\n\n# ====== Feature Extractor with Enhanced Model ======\nclass FeatureExtractor:\n    def __init__(self):\n        self.model = timm.create_model(Config.MODEL_NAME, pretrained=True)\n        self.model = self.model.to(device).eval()\n        # Freeze all layers\n        for param in self.model.parameters():\n            param.requires_grad = False\n        \n    @torch.no_grad()\n    def extract_embeddings(self, df, is_train=False):\n        dataset = AnimalDataset(df, is_train=is_train)\n        loader = DataLoader(dataset, batch_size=Config.BATCH_SIZE,\n                          num_workers=Config.NUM_WORKERS,\n                          pin_memory=True)\n        \n        embeddings = []\n        for batch in tqdm(loader, desc=\"Extracting embeddings\"):\n            embeddings.append(self.model(batch.to(device)).cpu())\n        return torch.cat(embeddings).numpy()\n\n# ====== Robust Data Splitting ======\n\ndef safe_train_test_split(db_df, val_size=0.2, min_samples=2, random_state=42):\n    \"\"\"Handle classes with insufficient samples and ensure all validation classes exist in training\"\"\"\n    # Step 1: Identify classes with enough samples\n    class_counts = db_df['identity'].value_counts()\n    valid_classes = class_counts[class_counts >= min_samples].index\n    \n    if len(valid_classes) == 0:\n        print(\"Warning: No classes have sufficient samples for validation split\")\n        return db_df.copy(), pd.DataFrame(columns=db_df.columns)\n    \n    # Step 2: First split to ensure all validation classes exist in training\n    train_classes, val_classes = train_test_split(\n        valid_classes,\n        test_size=val_size,\n        random_state=random_state\n    )\n    \n    # Step 3: Create splits\n    train_df = db_df[db_df['identity'].isin(train_classes)]\n    val_df = db_df[db_df['identity'].isin(val_classes)]\n    \n    # Step 4: Add rare classes to training\n    rare_df = db_df[~db_df['identity'].isin(valid_classes)]\n    if len(rare_df) > 0:\n        train_df = pd.concat([train_df, rare_df])\n        print(f\"Added {len(rare_df)} samples from rare classes to training\")\n    \n    return train_df, val_df\n \n# ====== Enhanced Evaluation Metrics ======\ndef evaluate_accuracy(query_embeddings, query_labels, db_embeddings, db_labels, threshold):\n    \"\"\"Calculate accuracy with confidence thresholding\"\"\"\n    # Normalize embeddings\n    db_embeddings = db_embeddings / np.linalg.norm(db_embeddings, axis=1, keepdims=True)\n    query_embeddings = query_embeddings / np.linalg.norm(query_embeddings, axis=1, keepdims=True)\n    \n    # Batch processing for efficiency\n    sim_matrix = cosine_similarity(query_embeddings, db_embeddings)\n    max_sims = np.max(sim_matrix, axis=1)\n    max_indices = np.argmax(sim_matrix, axis=1)\n    \n    correct = 0\n    total = 0\n    \n    for i in range(len(query_labels)):\n        if max_sims[i] >= threshold:\n            if db_labels[max_indices[i]] == query_labels[i]:\n                correct += 1\n        total += 1\n    \n    return correct / total if total > 0 else 0.0\n\n# ====== Main Execution with Improved Pipeline ======\nif __name__ == \"__main__\":\n    # Load and prepare data\n    df = pd.read_csv(Config.METADATA_PATH)\n    encoder = LabelEncoder()\n    extractor = FeatureExtractor()\n    \n    results = []\n    val_accuracies = []\n    class_reports = []\n    \n    for class_name in Config.CLASSES:\n        print(f\"\\n{'='*40}\\nProcessing {class_name}\\n{'='*40}\")\n        class_df = df[df[\"path\"].str.contains(class_name)]\n        db_df = class_df[class_df[\"split\"] == \"database\"].dropna(subset=[\"identity\"])\n        \n        if db_df.empty:\n            print(f\"No database samples found for {class_name}\")\n            continue\n            \n        # Split database into train and validation\n        train_df, val_df = safe_train_test_split(\n            db_df,\n            val_size=Config.VAL_SIZE,\n            min_samples=Config.MIN_SAMPLES_PER_CLASS,\n            random_state=Config.RANDOM_SEED\n        )\n        \n        if val_df.empty:\n            print(f\"Skipping validation for {class_name} - insufficient samples\")\n            val_accuracies.append(0)\n            continue\n         \n    \n        # Encode labels - fit on combined data first\n        all_identities = pd.concat([train_df['identity'], val_df['identity']]).unique()\n        encoder.fit(all_identities)\n        \n        train_df[\"label\"] = encoder.transform(train_df[\"identity\"])\n        val_df[\"label\"] = encoder.transform(val_df[\"identity\"])\n\n        \n        \n        # Extract embeddings\n        print(\"Extracting training embeddings...\")\n        train_embeddings = extractor.extract_embeddings(train_df, is_train=True)\n        print(\"Extracting validation embeddings...\")\n        val_embeddings = extractor.extract_embeddings(val_df, is_train=False)\n        \n        # Evaluate on validation set\n        val_accuracy = evaluate_accuracy(\n            val_embeddings, \n            val_df[\"label\"].values,\n            train_embeddings,\n            train_df[\"label\"].values,\n            Config.BASE_THRESHOLD\n        )\n        val_accuracies.append(val_accuracy)\n        print(f\"\\nValidation Accuracy for {class_name}: {val_accuracy:.4f}\")\n        \n        # Process queries if available\n        query_df = class_df[class_df[\"split\"] == \"query\"]\n        if not query_df.empty:\n            print(\"Processing query images...\")\n            query_embeddings = extractor.extract_embeddings(query_df, is_train=False)\n            query_embeddings = query_embeddings / np.linalg.norm(query_embeddings, axis=1, keepdims=True)\n            train_embeddings = train_embeddings / np.linalg.norm(train_embeddings, axis=1, keepdims=True)\n            \n            # Batch processing for queries\n            sim_matrix = cosine_similarity(query_embeddings, train_embeddings)\n            max_sims = np.max(sim_matrix, axis=1)\n            max_indices = np.argmax(sim_matrix, axis=1)\n            \n            for i in tqdm(range(len(query_df)), desc=\"Matching queries\"):\n                identity = (train_df.iloc[max_indices[i]][\"identity\"] \n                          if max_sims[i] >= Config.BASE_THRESHOLD \n                          else \"new_individual\")\n                results.append({\n                    \"image_id\": query_df.iloc[i][\"image_id\"],\n                    \"identity\": identity,\n                    \"confidence\": float(max_sims[i])\n                })\n    \n    # Save results and print summary\n    if results:\n        submission_df = pd.DataFrame(results)\n        submission_df.to_csv(\"submission.csv\", index=False)\n        print(\"\\nSubmission saved to submission.csv\")\n    \n    # Validation report\n    print(\"\\n\\nValidation Accuracy Summary:\")\n    for class_name, acc in zip(Config.CLASSES, val_accuracies):\n        print(f\"{class_name}: {acc:.4f}\")\n    \n    valid_accs = [acc for acc in val_accuracies if acc > 0]\n    if valid_accs:\n        print(f\"\\nMean Validation Accuracy: {np.mean(valid_accs):.4f}\")\n    else:\n        print(\"\\nNo valid validation results available\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T23:51:34.819273Z","iopub.execute_input":"2025-05-10T23:51:34.820093Z","iopub.status.idle":"2025-05-11T00:02:40.523640Z","shell.execute_reply.started":"2025-05-10T23:51:34.820064Z","shell.execute_reply":"2025-05-11T00:02:40.522585Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/2083387616.py:185: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  val_df[\"label\"] = encoder.transform(val_df[\"identity\"])\n","output_type":"stream"},{"name":"stdout","text":"\n========================================\nProcessing SeaTurtleID2022\n========================================\nAdded 1 samples from rare classes to training\nExtracting training embeddings...\n","output_type":"stream"},{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 106/106 [04:47<00:00,  2.71s/it]\n","output_type":"stream"},{"name":"stdout","text":"Extracting validation embeddings...\n","output_type":"stream"},{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 32/32 [01:25<00:00,  2.66s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nValidation Accuracy for SeaTurtleID2022: 0.0000\nProcessing query images...\n","output_type":"stream"},{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 8/8 [00:22<00:00,  2.87s/it]\nMatching queries: 100%|██████████| 500/500 [00:00<00:00, 21411.09it/s]\n/tmp/ipykernel_31/2083387616.py:185: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  val_df[\"label\"] = encoder.transform(val_df[\"identity\"])\n","output_type":"stream"},{"name":"stdout","text":"\n========================================\nProcessing LynxID2025\n========================================\nAdded 6 samples from rare classes to training\nExtracting training embeddings...\n","output_type":"stream"},{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 31/31 [01:26<00:00,  2.79s/it]\n","output_type":"stream"},{"name":"stdout","text":"Extracting validation embeddings...\n","output_type":"stream"},{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 16/16 [00:44<00:00,  2.77s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nValidation Accuracy for LynxID2025: 0.0000\nProcessing query images...\n","output_type":"stream"},{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 15/15 [00:44<00:00,  2.94s/it]\nMatching queries: 100%|██████████| 946/946 [00:00<00:00, 22625.37it/s]\n/tmp/ipykernel_31/2083387616.py:185: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  val_df[\"label\"] = encoder.transform(val_df[\"identity\"])\n","output_type":"stream"},{"name":"stdout","text":"\n========================================\nProcessing SalamanderID2025\n========================================\nAdded 310 samples from rare classes to training\nExtracting training embeddings...\n","output_type":"stream"},{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 19/19 [00:53<00:00,  2.81s/it]\n","output_type":"stream"},{"name":"stdout","text":"Extracting validation embeddings...\n","output_type":"stream"},{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 4/4 [00:13<00:00,  3.26s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nValidation Accuracy for SalamanderID2025: 0.0000\nProcessing query images...\n","output_type":"stream"},{"name":"stderr","text":"Extracting embeddings:  55%|█████▍    | 6/11 [00:22<00:19,  3.80s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/2083387616.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mquery_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Processing query images...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mquery_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m             \u001b[0mquery_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_embeddings\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mtrain_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_embeddings\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/2083387616.py\u001b[0m in \u001b[0;36mextract_embeddings\u001b[0;34m(self, df, is_train)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Extracting embeddings\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"23% Accuracy","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport timm\nimport joblib\n\n# ====== Configuration ======\nclass Config:\n    ROOT_DIR = \"/kaggle/input/animal-clef-2025\"\n    METADATA_PATH = os.path.join(ROOT_DIR, \"metadata.csv\")\n    EMBEDDING_DIM = 512\n    BATCH_SIZE = 64\n    NUM_WORKERS = 2\n    IMAGE_SIZE = 384\n    CLASSES = [\"SeaTurtleID2022\", \"LynxID2025\", \"SalamanderID2025\"]\n    MODEL_NAME = \"hf-hub:BVRA/MegaDescriptor-L-384\"\n    BASE_THRESHOLD = 0.87\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ====== Dataset and Model Setup ======\nclass AnimalDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df.reset_index(drop=True)\n        self.transform = transform or transforms.Compose([\n            transforms.Resize((Config.IMAGE_SIZE, Config.IMAGE_SIZE)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n        ])\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        img_path = os.path.join(Config.ROOT_DIR, self.df.iloc[idx]['path'])\n        try:\n            with Image.open(img_path) as img:\n                return self.transform(img.convert(\"RGB\"))\n        except:\n            return torch.zeros((3, Config.IMAGE_SIZE, Config.IMAGE_SIZE))\n\nclass FeatureExtractor:\n    def __init__(self):\n        self.model = timm.create_model(Config.MODEL_NAME, pretrained=True)\n        self.model = self.model.to(device).eval()\n        \n    @torch.no_grad()\n    def extract_embeddings(self, df):\n        dataset = AnimalDataset(df)\n        loader = DataLoader(dataset, batch_size=Config.BATCH_SIZE,\n                          num_workers=Config.NUM_WORKERS)\n        \n        embeddings = []\n        for batch in tqdm(loader, desc=\"Extracting embeddings\"):\n            embeddings.append(self.model(batch.to(device)).cpu())\n        return torch.cat(embeddings).numpy()\n\n# ====== Similarity Search ======\ndef find_matches(query_emb, db_embeddings, db_df, threshold):\n    \"\"\"NumPy implementation of similarity search\"\"\"\n    sims = cosine_similarity(query_emb.reshape(1, -1), db_embeddings)[0]\n    max_idx = np.argmax(sims)\n    return db_df.iloc[max_idx][\"identity\"] if sims[max_idx] >= threshold else \"new_individual\"\n\n# ====== Main Execution ======\nif __name__ == \"__main__\":\n    df = pd.read_csv(Config.METADATA_PATH)\n    extractor = FeatureExtractor()\n    results = []\n    \n    for class_name in Config.CLASSES:\n        print(f\"\\nProcessing {class_name}...\")\n        class_df = df[df[\"path\"].str.contains(class_name)]\n        db_df = class_df[class_df[\"split\"] == \"database\"].dropna(subset=[\"identity\"])\n        query_df = class_df[class_df[\"split\"] == \"query\"]\n        \n        if db_df.empty or query_df.empty:\n            continue\n            \n        db_embeddings = extractor.extract_embeddings(db_df)\n        query_embeddings = extractor.extract_embeddings(query_df)\n        \n        # Normalize embeddings\n        db_embeddings = db_embeddings / np.linalg.norm(db_embeddings, axis=1, keepdims=True)\n        query_embeddings = query_embeddings / np.linalg.norm(query_embeddings, axis=1, keepdims=True)\n        \n        for i, query_emb in enumerate(tqdm(query_embeddings, desc=\"Matching\")):\n            identity = find_matches(query_emb, db_embeddings, db_df, Config.BASE_THRESHOLD)\n            results.append({\n                \"image_id\": query_df.iloc[i][\"image_id\"],\n                \"identity\": identity\n            })\n    \n    pd.DataFrame(results).to_csv(\"submission.csv\", index=False)\n    print(\"Done! Results saved to submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T23:38:28.527124Z","iopub.execute_input":"2025-05-10T23:38:28.527822Z","iopub.status.idle":"2025-05-10T23:50:36.356208Z","shell.execute_reply.started":"2025-05-10T23:38:28.527798Z","shell.execute_reply":"2025-05-10T23:50:36.355351Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d63d4ea2a6294ecda33a9bdeacdadd6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99e1efbe67cb46ea85c510edf83097fb"}},"metadata":{}},{"name":"stdout","text":"\nProcessing SeaTurtleID2022...\n","output_type":"stream"},{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 137/137 [06:17<00:00,  2.76s/it]\nExtracting embeddings: 100%|██████████| 8/8 [00:23<00:00,  2.88s/it]\nMatching: 100%|██████████| 500/500 [00:19<00:00, 25.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nProcessing LynxID2025...\n","output_type":"stream"},{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 47/47 [02:09<00:00,  2.76s/it]\nExtracting embeddings: 100%|██████████| 15/15 [00:43<00:00,  2.93s/it]\nMatching: 100%|██████████| 946/946 [00:12<00:00, 76.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nProcessing SalamanderID2025...\n","output_type":"stream"},{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 22/22 [01:02<00:00,  2.85s/it]\nExtracting embeddings: 100%|██████████| 11/11 [00:32<00:00,  2.98s/it]\nMatching: 100%|██████████| 689/689 [00:03<00:00, 197.98it/s]","output_type":"stream"},{"name":"stdout","text":"Done! Results saved to submission.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"27% Accuracy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport timm\nimport joblib\n\n# ====== Configuration ======\nclass Config:\n    ROOT_DIR = \"/kaggle/input/animal-clef-2025\"\n    METADATA_PATH = os.path.join(ROOT_DIR, \"metadata.csv\")\n    EMBEDDING_DIM = 512\n    BATCH_SIZE = 96\n    NUM_WORKERS = 2\n    IMAGE_SIZE = 384\n    CLASSES = [\"SeaTurtleID2022\", \"LynxID2025\", \"SalamanderID2025\"]\n    MODEL_NAME = \"hf-hub:BVRA/MegaDescriptor-L-384\"\n    BASE_THRESHOLD = 0.85\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ====== Dataset and Model Setup ======\nclass AnimalDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df.reset_index(drop=True)\n        self.transform = transform or transforms.Compose([\n            transforms.Resize((Config.IMAGE_SIZE, Config.IMAGE_SIZE)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n        ])\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        img_path = os.path.join(Config.ROOT_DIR, self.df.iloc[idx]['path'])\n        try:\n            with Image.open(img_path) as img:\n                return self.transform(img.convert(\"RGB\"))\n        except:\n            return torch.zeros((3, Config.IMAGE_SIZE, Config.IMAGE_SIZE))\n\nclass FeatureExtractor:\n    def __init__(self):\n        self.model = timm.create_model(Config.MODEL_NAME, pretrained=True)\n        self.model = self.model.to(device).eval()\n        \n    @torch.no_grad()\n    def extract_embeddings(self, df):\n        dataset = AnimalDataset(df)\n        loader = DataLoader(dataset, batch_size=Config.BATCH_SIZE,\n                          num_workers=Config.NUM_WORKERS)\n        \n        embeddings = []\n        for batch in tqdm(loader, desc=\"Extracting embeddings\"):\n            embeddings.append(self.model(batch.to(device)).cpu())\n        return torch.cat(embeddings).numpy()\n\n# ====== Similarity Search ======\ndef find_matches(query_emb, db_embeddings, db_df, threshold):\n    \"\"\"NumPy implementation of similarity search\"\"\"\n    sims = cosine_similarity(query_emb.reshape(1, -1), db_embeddings)[0]\n    max_idx = np.argmax(sims)\n    return db_df.iloc[max_idx][\"identity\"] if sims[max_idx] >= threshold else \"new_individual\"\n\n# ====== Main Execution ======\nif __name__ == \"__main__\":\n    df = pd.read_csv(Config.METADATA_PATH)\n    extractor = FeatureExtractor()\n    results = []\n    \n    for class_name in Config.CLASSES:\n        print(f\"\\nProcessing {class_name}...\")\n        class_df = df[df[\"path\"].str.contains(class_name)]\n        db_df = class_df[class_df[\"split\"] == \"database\"].dropna(subset=[\"identity\"])\n        query_df = class_df[class_df[\"split\"] == \"query\"]\n        \n        if db_df.empty or query_df.empty:\n            continue\n            \n        db_embeddings = extractor.extract_embeddings(db_df)\n        query_embeddings = extractor.extract_embeddings(query_df)\n        \n        # Normalize embeddings\n        db_embeddings = db_embeddings / np.linalg.norm(db_embeddings, axis=1, keepdims=True)\n        query_embeddings = query_embeddings / np.linalg.norm(query_embeddings, axis=1, keepdims=True)\n        \n        for i, query_emb in enumerate(tqdm(query_embeddings, desc=\"Matching\")):\n            identity = find_matches(query_emb, db_embeddings, db_df, Config.BASE_THRESHOLD)\n            results.append({\n                \"image_id\": query_df.iloc[i][\"image_id\"],\n                \"identity\": identity\n            })\n    \n    pd.DataFrame(results).to_csv(\"submission.csv\", index=False)\n    print(\"Done! Results saved to submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T00:03:43.625017Z","iopub.execute_input":"2025-05-11T00:03:43.625754Z","iopub.status.idle":"2025-05-11T00:15:37.633285Z","shell.execute_reply.started":"2025-05-11T00:03:43.625729Z","shell.execute_reply":"2025-05-11T00:15:37.632415Z"}},"outputs":[{"name":"stdout","text":"\nProcessing SeaTurtleID2022...\n","output_type":"stream"},{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 91/91 [06:19<00:00,  4.17s/it]\nExtracting embeddings: 100%|██████████| 6/6 [00:23<00:00,  3.88s/it]\nMatching: 100%|██████████| 500/500 [00:18<00:00, 26.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nProcessing LynxID2025...\n","output_type":"stream"},{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 31/31 [02:10<00:00,  4.21s/it]\nExtracting embeddings: 100%|██████████| 10/10 [00:44<00:00,  4.45s/it]\nMatching: 100%|██████████| 946/946 [00:11<00:00, 82.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nProcessing SalamanderID2025...\n","output_type":"stream"},{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 15/15 [01:03<00:00,  4.24s/it]\nExtracting embeddings: 100%|██████████| 8/8 [00:33<00:00,  4.18s/it]\nMatching: 100%|██████████| 689/689 [00:03<00:00, 196.93it/s]","output_type":"stream"},{"name":"stdout","text":"Done! Results saved to submission.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.model_selection import train_test_split\nimport timm\nimport joblib\n\n# ====== Configuration ======\nclass Config:\n    ROOT_DIR = \"/kaggle/input/animal-clef-2025\"\n    METADATA_PATH = os.path.join(ROOT_DIR, \"metadata.csv\")\n    EMBEDDING_DIM = 512\n    BATCH_SIZE = 96\n    NUM_WORKERS = 2\n    IMAGE_SIZE = 384\n    CLASSES = [\"SeaTurtleID2022\", \"LynxID2025\", \"SalamanderID2025\"]\n    MODEL_NAME = \"hf-hub:BVRA/MegaDescriptor-L-384\"\n    BASE_THRESHOLD = 0.85\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ====== Dataset and Model Setup ======\nclass AnimalDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df.reset_index(drop=True)\n        self.transform = transform or transforms.Compose([\n            transforms.Resize((Config.IMAGE_SIZE, Config.IMAGE_SIZE)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n        ])\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        img_path = os.path.join(Config.ROOT_DIR, self.df.iloc[idx]['path'])\n        try:\n            with Image.open(img_path) as img:\n                return self.transform(img.convert(\"RGB\"))\n        except:\n            return torch.zeros((3, Config.IMAGE_SIZE, Config.IMAGE_SIZE))\n\nclass FeatureExtractor:\n    def __init__(self):\n        self.model = timm.create_model(Config.MODEL_NAME, pretrained=True)\n        self.model = self.model.to(device).eval()\n        \n    @torch.no_grad()\n    def extract_embeddings(self, df):\n        dataset = AnimalDataset(df)\n        loader = DataLoader(dataset, batch_size=Config.BATCH_SIZE,\n                            num_workers=Config.NUM_WORKERS)\n        \n        embeddings = []\n        for batch in tqdm(loader, desc=\"Extracting embeddings\"):\n            embeddings.append(self.model(batch.to(device)).cpu())\n        return torch.cat(embeddings).numpy()\n\n# ====== Similarity Search ======\ndef find_matches(query_emb, db_embeddings, db_df, threshold):\n    \"\"\"NumPy implementation of similarity search\"\"\"\n    sims = cosine_similarity(query_emb.reshape(1, -1), db_embeddings)[0]\n    max_idx = np.argmax(sims)\n    return db_df.iloc[max_idx][\"identity\"] if sims[max_idx] >= threshold else \"new_individual\"\n\n# ====== Safe Stratified Split ======\ndef safe_stratified_split(df, test_size=0.2, seed=42):\n    train_indices = []\n    val_indices = []\n\n    grouped = df.groupby(\"identity\").indices\n    for label, indices in grouped.items():\n        indices = list(indices)\n        if len(indices) == 1:\n            train_indices.append(indices[0])  # Cannot split\n        else:\n            tr_idx, val_idx = train_test_split(indices, test_size=test_size, random_state=seed)\n            train_indices.extend(tr_idx)\n            val_indices.extend(val_idx)\n    return df.iloc[train_indices].reset_index(drop=True), df.iloc[val_indices].reset_index(drop=True)\n\n# ====== Main Execution ======\nif __name__ == \"__main__\":\n    df = pd.read_csv(Config.METADATA_PATH)\n    extractor = FeatureExtractor()\n    submission_results = []\n\n    for class_name in Config.CLASSES:\n        print(f\"\\nProcessing {class_name}...\")\n        class_df = df[df[\"path\"].str.contains(class_name)]\n        full_db_df = class_df[class_df[\"split\"] == \"database\"].dropna(subset=[\"identity\"])\n        query_df = class_df[class_df[\"split\"] == \"query\"]\n\n        if full_db_df.empty or query_df.empty:\n            continue\n\n        # Split database into training and validation\n        db_df, val_df = safe_stratified_split(full_db_df)\n\n        print(f\"DB: {len(db_df)}, VAL: {len(val_df)}, QUERY: {len(query_df)}\")\n\n        # Extract embeddings\n        db_embeddings = extractor.extract_embeddings(db_df)\n        val_embeddings = extractor.extract_embeddings(val_df)\n        query_embeddings = extractor.extract_embeddings(query_df)\n\n        # Normalize\n        db_embeddings = db_embeddings / np.linalg.norm(db_embeddings, axis=1, keepdims=True)\n        val_embeddings = val_embeddings / np.linalg.norm(val_embeddings, axis=1, keepdims=True)\n        query_embeddings = query_embeddings / np.linalg.norm(query_embeddings, axis=1, keepdims=True)\n\n        # ====== Validation Evaluation ======\n        correct = 0\n        total = len(val_df)\n        for i, val_emb in enumerate(tqdm(val_embeddings, desc=f\"Validating {class_name}\")):\n            predicted = find_matches(val_emb, db_embeddings, db_df, Config.BASE_THRESHOLD)\n            true_id = val_df.iloc[i][\"identity\"]\n            if predicted == true_id:\n                correct += 1\n        acc = correct / total if total > 0 else 0.0\n        print(f\"Validation Accuracy for {class_name}: {acc:.4f} ({correct}/{total})\")\n\n        # ====== Query Predictions for Submission ======\n        for i, query_emb in enumerate(tqdm(query_embeddings, desc=f\"Matching Query {class_name}\")):\n            identity = find_matches(query_emb, db_embeddings, db_df, Config.BASE_THRESHOLD)\n            submission_results.append({\n                \"image_id\": query_df.iloc[i][\"image_id\"],\n                \"identity\": identity\n            })\n\n    pd.DataFrame(submission_results).to_csv(\"submission.csv\", index=False)\n    print(\"Done! Submission results saved to submission.csv.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T00:35:34.033177Z","iopub.execute_input":"2025-05-11T00:35:34.033530Z","iopub.status.idle":"2025-05-11T00:48:40.427200Z","shell.execute_reply.started":"2025-05-11T00:35:34.033500Z","shell.execute_reply":"2025-05-11T00:48:40.426293Z"}},"outputs":[{"name":"stdout","text":"\nProcessing SeaTurtleID2022...\nDB: 6808, VAL: 1921, QUERY: 500\n","output_type":"stream"},{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 71/71 [04:57<00:00,  4.19s/it]\nExtracting embeddings: 100%|██████████| 21/21 [01:24<00:00,  4.03s/it]\nExtracting embeddings: 100%|██████████| 6/6 [00:23<00:00,  3.87s/it]\nValidating SeaTurtleID2022: 100%|██████████| 1921/1921 [01:01<00:00, 31.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy for SeaTurtleID2022: 0.6158 (1183/1921)\n","output_type":"stream"},{"name":"stderr","text":"Matching Query SeaTurtleID2022: 100%|██████████| 500/500 [00:15<00:00, 31.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nProcessing LynxID2025...\nDB: 2339, VAL: 618, QUERY: 946\n","output_type":"stream"},{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 25/25 [01:43<00:00,  4.13s/it]\nExtracting embeddings: 100%|██████████| 7/7 [00:28<00:00,  4.14s/it]\nExtracting embeddings: 100%|██████████| 10/10 [00:44<00:00,  4.46s/it]\nValidating LynxID2025: 100%|██████████| 618/618 [00:06<00:00, 97.49it/s] \n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy for LynxID2025: 0.0696 (43/618)\n","output_type":"stream"},{"name":"stderr","text":"Matching Query LynxID2025: 100%|██████████| 946/946 [00:09<00:00, 96.95it/s] \n","output_type":"stream"},{"name":"stdout","text":"\nProcessing SalamanderID2025...\nDB: 1059, VAL: 329, QUERY: 689\n","output_type":"stream"},{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 12/12 [00:49<00:00,  4.15s/it]\nExtracting embeddings: 100%|██████████| 4/4 [00:17<00:00,  4.49s/it]\nExtracting embeddings: 100%|██████████| 8/8 [00:33<00:00,  4.20s/it]\nValidating SalamanderID2025: 100%|██████████| 329/329 [00:01<00:00, 257.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy for SalamanderID2025: 0.1216 (40/329)\n","output_type":"stream"},{"name":"stderr","text":"Matching Query SalamanderID2025: 100%|██████████| 689/689 [00:02<00:00, 261.61it/s]","output_type":"stream"},{"name":"stdout","text":"Done! Submission results saved to submission.csv.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.model_selection import train_test_split\nimport timm\nimport joblib\n\n# ====== Configuration ======\nclass Config:\n    ROOT_DIR = \"/kaggle/input/animal-clef-2025\"\n    METADATA_PATH = os.path.join(ROOT_DIR, \"metadata.csv\")\n    EMBEDDING_DIM = 512\n    BATCH_SIZE = 96\n    NUM_WORKERS = 2\n    IMAGE_SIZE = 384\n    CLASSES = [\"SeaTurtleID2022\", \"LynxID2025\", \"SalamanderID2025\"]\n    MODEL_NAME = \"hf-hub:BVRA/MegaDescriptor-L-384\"\n    BASE_THRESHOLD = 0.85\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ====== Dataset and Model Setup ======\nclass AnimalDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df.reset_index(drop=True)\n        self.transform = transform or transforms.Compose([\n            transforms.Resize((Config.IMAGE_SIZE, Config.IMAGE_SIZE)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n        ])\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        img_path = os.path.join(Config.ROOT_DIR, self.df.iloc[idx]['path'])\n        try:\n            with Image.open(img_path) as img:\n                return self.transform(img.convert(\"RGB\"))\n        except:\n            return torch.zeros((3, Config.IMAGE_SIZE, Config.IMAGE_SIZE))\n\nclass FeatureExtractor:\n    def __init__(self):\n        self.model = timm.create_model(Config.MODEL_NAME, pretrained=True)\n        self.model = self.model.to(device).eval()\n        \n    @torch.no_grad()\n    def extract_embeddings(self, df):\n        dataset = AnimalDataset(df)\n        loader = DataLoader(dataset, batch_size=Config.BATCH_SIZE,\n                            num_workers=Config.NUM_WORKERS)\n        \n        embeddings = []\n        for batch in tqdm(loader, desc=\"Extracting embeddings\"):\n            embeddings.append(self.model(batch.to(device)).cpu())\n        return torch.cat(embeddings).numpy()\n\n# ====== Similarity Search ======\ndef find_matches(query_emb, db_embeddings, db_df, threshold):\n    \"\"\"NumPy implementation of similarity search\"\"\"\n    sims = cosine_similarity(query_emb.reshape(1, -1), db_embeddings)[0]\n    max_idx = np.argmax(sims)\n    return db_df.iloc[max_idx][\"identity\"] if sims[max_idx] >= threshold else \"new_individual\"\n\ndef find_best_threshold(val_embeddings, val_df, db_embeddings, db_df):\n    best_thresh = 0.0\n    best_acc = 0.0\n    for t in np.arange(0.5, 0.96, 0.01):\n        correct = 0\n        for i, emb in enumerate(val_embeddings):\n            pred = find_matches(emb, db_embeddings, db_df, t)\n            if pred == val_df.iloc[i][\"identity\"]:\n                correct += 1\n        acc = correct / len(val_df)\n        if acc > best_acc:\n            best_acc = acc\n            best_thresh = t\n    return best_thresh, best_acc\n\n\n# ====== Safe Stratified Split ======\ndef safe_stratified_split(df, test_size=0.2, seed=42):\n    train_indices = []\n    val_indices = []\n\n    grouped = df.groupby(\"identity\").indices\n    for label, indices in grouped.items():\n        indices = list(indices)\n        if len(indices) == 1:\n            train_indices.append(indices[0])  # Cannot split\n        else:\n            tr_idx, val_idx = train_test_split(indices, test_size=test_size, random_state=seed)\n            train_indices.extend(tr_idx)\n            val_indices.extend(val_idx)\n    return df.iloc[train_indices].reset_index(drop=True), df.iloc[val_indices].reset_index(drop=True)\n\n \n# ====== Main Execution ======\nif __name__ == \"__main__\":\n    df = pd.read_csv(Config.METADATA_PATH)\n    extractor = FeatureExtractor()\n    submission_results = []\n    thresholds = {}\n\n    for class_name in Config.CLASSES:\n        print(f\"\\nProcessing {class_name}...\")\n        class_df = df[df[\"path\"].str.contains(class_name)]\n        full_db_df = class_df[class_df[\"split\"] == \"database\"].dropna(subset=[\"identity\"])\n        query_df = class_df[class_df[\"split\"] == \"query\"]\n\n        if full_db_df.empty or query_df.empty:\n            continue\n\n        # Split database into training and validation\n        db_df, val_df = safe_stratified_split(full_db_df)\n\n        print(f\"DB: {len(db_df)}, VAL: {len(val_df)}, QUERY: {len(query_df)}\")\n\n        # Extract embeddings\n        db_embeddings = extractor.extract_embeddings(db_df)\n        val_embeddings = extractor.extract_embeddings(val_df)\n        query_embeddings = extractor.extract_embeddings(query_df)\n\n        # Normalize\n        db_embeddings = db_embeddings / np.linalg.norm(db_embeddings, axis=1, keepdims=True)\n        val_embeddings = val_embeddings / np.linalg.norm(val_embeddings, axis=1, keepdims=True)\n        query_embeddings = query_embeddings / np.linalg.norm(query_embeddings, axis=1, keepdims=True)\n\n        # ====== Threshold Tuning ======\n        best_thresh, best_acc = find_best_threshold(\n            val_embeddings, val_df, db_embeddings, db_df\n        )\n        thresholds[class_name] = best_thresh\n        print(f\"Validation Accuracy for {class_name}: {best_acc:.4f} with threshold {best_thresh:.2f}\")\n\n        # ====== Query Predictions for Submission ======\n        for i, query_emb in enumerate(tqdm(query_embeddings, desc=f\"Matching Query {class_name}\")):\n            identity = find_matches(query_emb, db_embeddings, db_df, threshold=best_thresh)\n            submission_results.append({\n                \"image_id\": query_df.iloc[i][\"image_id\"],\n                \"identity\": identity\n            })\n\n    # Save submission\n    pd.DataFrame(submission_results).to_csv(\"submission.csv\", index=False)\n    print(\"Done! Submission results saved to submission.csv.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T00:56:28.286755Z","iopub.execute_input":"2025-05-11T00:56:28.287568Z"}},"outputs":[{"name":"stdout","text":"\nProcessing SeaTurtleID2022...\nDB: 6808, VAL: 1921, QUERY: 500\n","output_type":"stream"},{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 71/71 [04:56<00:00,  4.18s/it]\nExtracting embeddings: 100%|██████████| 21/21 [01:24<00:00,  4.04s/it]\nExtracting embeddings: 100%|██████████| 6/6 [00:23<00:00,  3.92s/it]\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport timm\nimport joblib\n\n# ====== Configuration ======\nclass Config:\n    ROOT_DIR = \"/kaggle/input/animal-clef-2025\"\n    METADATA_PATH = os.path.join(ROOT_DIR, \"metadata.csv\")\n    EMBEDDING_DIM = 512\n    BATCH_SIZE = 96\n    NUM_WORKERS = 2\n    IMAGE_SIZE = 384\n    CLASSES = [\"SeaTurtleID2022\", \"LynxID2025\", \"SalamanderID2025\"]\n    MODEL_NAME = \"hf-hub:BVRA/MegaDescriptor-L-384\"\n    BASE_THRESHOLD = 0.85\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ====== Dataset and Model Setup ======\nclass AnimalDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df.reset_index(drop=True)\n        self.transform = transform or transforms.Compose([\n            transforms.Resize((Config.IMAGE_SIZE, Config.IMAGE_SIZE)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n        ])\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        img_path = os.path.join(Config.ROOT_DIR, self.df.iloc[idx]['path'])\n        try:\n            with Image.open(img_path) as img:\n                return self.transform(img.convert(\"RGB\"))\n        except:\n            return torch.zeros((3, Config.IMAGE_SIZE, Config.IMAGE_SIZE))\n\nclass FeatureExtractor:\n    def __init__(self):\n        self.model = timm.create_model(Config.MODEL_NAME, pretrained=True)\n        self.model = self.model.to(device).eval()\n        \n    @torch.no_grad()\n    def extract_embeddings(self, df):\n        dataset = AnimalDataset(df)\n        loader = DataLoader(dataset, batch_size=Config.BATCH_SIZE,\n                          num_workers=Config.NUM_WORKERS)\n        \n        embeddings = []\n        for batch in tqdm(loader, desc=\"Extracting embeddings\"):\n            embeddings.append(self.model(batch.to(device)).cpu())\n        return torch.cat(embeddings).numpy()\n\n# ====== Similarity Search ======\ndef find_matches(query_emb, db_embeddings, db_df, threshold):\n    \"\"\"NumPy implementation of similarity search\"\"\"\n    sims = cosine_similarity(query_emb.reshape(1, -1), db_embeddings)[0]\n    max_idx = np.argmax(sims)\n    return db_df.iloc[max_idx][\"identity\"] if sims[max_idx] >= threshold else \"new_individual\"\n\n# ====== Main Execution ======\nif __name__ == \"__main__\":\n    df = pd.read_csv(Config.METADATA_PATH)\n    extractor = FeatureExtractor()\n    results = []\n    \n    for class_name in Config.CLASSES:\n        print(f\"\\nProcessing {class_name}...\")\n        class_df = df[df[\"path\"].str.contains(class_name)]\n        db_df = class_df[class_df[\"split\"] == \"database\"].dropna(subset=[\"identity\"])\n        query_df = class_df[class_df[\"split\"] == \"query\"]\n        \n        if db_df.empty or query_df.empty:\n            continue\n            \n        db_embeddings = extractor.extract_embeddings(db_df)\n        query_embeddings = extractor.extract_embeddings(query_df)\n        \n        # Normalize embeddings\n        db_embeddings = db_embeddings / np.linalg.norm(db_embeddings, axis=1, keepdims=True)\n        query_embeddings = query_embeddings / np.linalg.norm(query_embeddings, axis=1, keepdims=True)\n        \n        for i, query_emb in enumerate(tqdm(query_embeddings, desc=\"Matching\")):\n            identity = find_matches(query_emb, db_embeddings, db_df, Config.BASE_THRESHOLD)\n            results.append({\n                \"image_id\": query_df.iloc[i][\"image_id\"],\n                \"identity\": identity\n            })\n    \n    pd.DataFrame(results).to_csv(\"submission.csv\", index=False)\n    print(\"Done! Results saved to submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T00:22:34.785652Z","iopub.execute_input":"2025-05-11T00:22:34.786262Z","iopub.status.idle":"2025-05-11T00:34:56.612841Z","shell.execute_reply.started":"2025-05-11T00:22:34.786227Z","shell.execute_reply":"2025-05-11T00:34:56.611760Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be83d8b8452847519d63995654d04493"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa9ac3ba32884a8e91fcae9f7446ffd1"}},"metadata":{}},{"name":"stdout","text":"\nProcessing SeaTurtleID2022...\n","output_type":"stream"},{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 91/91 [06:20<00:00,  4.18s/it]\nExtracting embeddings: 100%|██████████| 6/6 [00:23<00:00,  3.98s/it]\nMatching: 100%|██████████| 500/500 [00:20<00:00, 24.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nProcessing LynxID2025...\n","output_type":"stream"},{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 31/31 [02:11<00:00,  4.25s/it]\nExtracting embeddings: 100%|██████████| 10/10 [00:45<00:00,  4.57s/it]\nMatching: 100%|██████████| 946/946 [00:13<00:00, 71.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nProcessing SalamanderID2025...\n","output_type":"stream"},{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 15/15 [01:04<00:00,  4.29s/it]\nExtracting embeddings: 100%|██████████| 8/8 [00:34<00:00,  4.32s/it]\nMatching: 100%|██████████| 689/689 [00:03<00:00, 177.14it/s]","output_type":"stream"},{"name":"stdout","text":"Done! Results saved to submission.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.model_selection import train_test_split\nimport timm\nimport joblib\nfrom collections import defaultdict\n\n# ====== Configuration ======\nclass Config:\n    ROOT_DIR = \"/kaggle/input/animal-clef-2025\"\n    METADATA_PATH = os.path.join(ROOT_DIR, \"metadata.csv\")\n    EMBEDDING_DIM = 512\n    BATCH_SIZE = 128  # Increased batch size\n    NUM_WORKERS = 4\n    IMAGE_SIZE = 384\n    CLASSES = [\"SeaTurtleID2022\", \"LynxID2025\", \"SalamanderID2025\"]\n    MODEL_NAME = \"hf-hub:BVRA/MegaDescriptor-L-384\"\n    BASE_THRESHOLD = 0.87  # Adjusted threshold\n    VAL_SIZE = 0.2\n    RANDOM_SEED = 42\n    MIN_SAMPLES_PER_CLASS = 2  # Minimum samples required per class\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ====== Dataset Class with Enhanced Augmentation ======\nclass AnimalDataset(Dataset):\n    def __init__(self, df, transform=None, is_train=True):\n        self.df = df.reset_index(drop=True)\n        self.is_train = is_train\n        self.transform = transform or self.get_default_transform(is_train)\n        \n    def get_default_transform(self, is_train):\n        if is_train:\n            return transforms.Compose([\n                transforms.Resize((Config.IMAGE_SIZE, Config.IMAGE_SIZE)),\n                #transforms.RandomHorizontalFlip(),\n                #transforms.RandomAffine(degrees=10, translate=(0.1, 0.1)),\n                #transforms.ColorJitter(brightness=0.2, contrast=0.2),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n            ])\n        else:\n            return transforms.Compose([\n                transforms.Resize((Config.IMAGE_SIZE, Config.IMAGE_SIZE)),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n            ])\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        img_path = os.path.join(Config.ROOT_DIR, self.df.iloc[idx]['path'])\n        try:\n            with Image.open(img_path) as img:\n                if self.transform:\n                    img = self.transform(img.convert(\"RGB\"))\n                return img\n        except Exception as e:\n            print(f\"Error loading {img_path}: {str(e)}\")\n            return torch.zeros((3, Config.IMAGE_SIZE, Config.IMAGE_SIZE))\n\n# ====== Feature Extractor with Enhanced Model ======\nclass FeatureExtractor:\n    def __init__(self):\n        self.model = timm.create_model(Config.MODEL_NAME, pretrained=True)\n        self.model = self.model.to(device).eval()\n        # Freeze all layers\n        for param in self.model.parameters():\n            param.requires_grad = False\n        \n    @torch.no_grad()\n    def extract_embeddings(self, df, is_train=False):\n        dataset = AnimalDataset(df, is_train=is_train)\n        loader = DataLoader(dataset, batch_size=Config.BATCH_SIZE,\n                          num_workers=Config.NUM_WORKERS,\n                          pin_memory=True)\n        \n        embeddings = []\n        for batch in tqdm(loader, desc=\"Extracting embeddings\"):\n            embeddings.append(self.model(batch.to(device)).cpu())\n        return torch.cat(embeddings).numpy()\n\n# ====== Robust Data Splitting ======\ndef safe_train_test_split(db_df, val_size=0.2, min_samples=2, random_state=42):\n    \"\"\"Handle classes with insufficient samples\"\"\"\n    class_counts = db_df['identity'].value_counts()\n    valid_classes = class_counts[class_counts >= min_samples].index\n    \n    if len(valid_classes) == 0:\n        print(\"Warning: No classes have sufficient samples for validation split\")\n        return db_df.copy(), pd.DataFrame(columns=db_df.columns)\n    \n    valid_df = db_df[db_df['identity'].isin(valid_classes)]\n    train_df, val_df = train_test_split(\n        valid_df,\n        test_size=val_size,\n        stratify=valid_df['identity'],\n        random_state=random_state\n    )\n    \n    # Add insufficient samples to training\n    insufficient_df = db_df[~db_df['identity'].isin(valid_classes)]\n    if len(insufficient_df) > 0:\n        train_df = pd.concat([train_df, insufficient_df])\n        print(f\"Added {len(insufficient_df)} samples from rare classes to training\")\n    \n    return train_df, val_df\n\n# ====== Enhanced Evaluation Metrics ======\ndef evaluate_accuracy(query_embeddings, query_labels, db_embeddings, db_labels, threshold):\n    \"\"\"Calculate accuracy with confidence thresholding\"\"\"\n    # Normalize embeddings\n    db_embeddings = db_embeddings / np.linalg.norm(db_embeddings, axis=1, keepdims=True)\n    query_embeddings = query_embeddings / np.linalg.norm(query_embeddings, axis=1, keepdims=True)\n    \n    # Batch processing for efficiency\n    sim_matrix = cosine_similarity(query_embeddings, db_embeddings)\n    max_sims = np.max(sim_matrix, axis=1)\n    max_indices = np.argmax(sim_matrix, axis=1)\n    \n    correct = 0\n    total = 0\n    \n    for i in range(len(query_labels)):\n        if max_sims[i] >= threshold:\n            if db_labels[max_indices[i]] == query_labels[i]:\n                correct += 1\n        total += 1\n    \n    return correct / total if total > 0 else 0.0\n\n# ====== Main Execution with Improved Pipeline ======\nif __name__ == \"__main__\":\n    # Load and prepare data\n    df = pd.read_csv(Config.METADATA_PATH)\n    encoder = LabelEncoder()\n    extractor = FeatureExtractor()\n    \n    results = []\n    val_accuracies = []\n    class_reports = []\n    \n    for class_name in Config.CLASSES:\n        print(f\"\\n{'='*40}\\nProcessing {class_name}\\n{'='*40}\")\n        class_df = df[df[\"path\"].str.contains(class_name)]\n        db_df = class_df[class_df[\"split\"] == \"database\"].dropna(subset=[\"identity\"])\n        \n        if db_df.empty:\n            print(f\"No database samples found for {class_name}\")\n            continue\n            \n        # Split database into train and validation\n        train_df, val_df = safe_train_test_split(\n            db_df,\n            val_size=Config.VAL_SIZE,\n            min_samples=Config.MIN_SAMPLES_PER_CLASS,\n            random_state=Config.RANDOM_SEED\n        )\n        \n        if val_df.empty:\n            print(f\"Skipping validation for {class_name} - insufficient samples\")\n            val_accuracies.append(0)\n            continue\n            \n        # Encode labels\n        train_df[\"label\"] = encoder.fit_transform(train_df[\"identity\"])\n        val_df[\"label\"] = encoder.transform(val_df[\"identity\"])\n        \n        # Extract embeddings\n        print(\"Extracting training embeddings...\")\n        train_embeddings = extractor.extract_embeddings(train_df, is_train=True)\n        print(\"Extracting validation embeddings...\")\n        val_embeddings = extractor.extract_embeddings(val_df, is_train=False)\n        \n        # Evaluate on validation set\n        val_accuracy = evaluate_accuracy(\n            val_embeddings, \n            val_df[\"label\"].values,\n            train_embeddings,\n            train_df[\"label\"].values,\n            Config.BASE_THRESHOLD\n        )\n        val_accuracies.append(val_accuracy)\n        print(f\"\\nValidation Accuracy for {class_name}: {val_accuracy:.4f}\")\n        \n        # Process queries if available\n        query_df = class_df[class_df[\"split\"] == \"query\"]\n        if not query_df.empty:\n            print(\"Processing query images...\")\n            query_embeddings = extractor.extract_embeddings(query_df, is_train=False)\n            query_embeddings = query_embeddings / np.linalg.norm(query_embeddings, axis=1, keepdims=True)\n            train_embeddings = train_embeddings / np.linalg.norm(train_embeddings, axis=1, keepdims=True)\n            \n            # Batch processing for queries\n            sim_matrix = cosine_similarity(query_embeddings, train_embeddings)\n            max_sims = np.max(sim_matrix, axis=1)\n            max_indices = np.argmax(sim_matrix, axis=1)\n            \n            for i in tqdm(range(len(query_df)), desc=\"Matching queries\"):\n                identity = (train_df.iloc[max_indices[i]][\"identity\"] \n                          if max_sims[i] >= Config.BASE_THRESHOLD \n                          else \"new_individual\")\n                results.append({\n                    \"image_id\": query_df.iloc[i][\"image_id\"],\n                    \"identity\": identity,\n                    \"confidence\": float(max_sims[i])\n                })\n    \n    # Save results and print summary\n    if results:\n        submission_df = pd.DataFrame(results)\n        submission_df.to_csv(\"submission.csv\", index=False)\n        print(\"\\nSubmission saved to submission.csv\")\n    \n    # Validation report\n    print(\"\\n\\nValidation Accuracy Summary:\")\n    for class_name, acc in zip(Config.CLASSES, val_accuracies):\n        print(f\"{class_name}: {acc:.4f}\")\n    \n    valid_accs = [acc for acc in val_accuracies if acc > 0]\n    if valid_accs:\n        print(f\"\\nMean Validation Accuracy: {np.mean(valid_accs):.4f}\")\n    else:\n        print(\"\\nNo valid validation results available\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T00:16:54.669196Z","iopub.execute_input":"2025-05-11T00:16:54.669527Z","iopub.status.idle":"2025-05-11T00:17:05.594930Z","shell.execute_reply.started":"2025-05-11T00:16:54.669502Z","shell.execute_reply":"2025-05-11T00:17:05.593670Z"}},"outputs":[{"name":"stdout","text":"\n========================================\nProcessing SeaTurtleID2022\n========================================\nAdded 1 samples from rare classes to training\nExtracting training embeddings...\n","output_type":"stream"},{"name":"stderr","text":"Extracting embeddings:   0%|          | 0/55 [00:05<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/3505525042.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;31m# Extract embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extracting training embeddings...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mtrain_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extracting validation embeddings...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mval_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/3505525042.py\u001b[0m in \u001b[0;36mextract_embeddings\u001b[0;34m(self, df, is_train)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Extracting embeddings\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/models/swin_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    839\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/models/swin_transformer.py\u001b[0m in \u001b[0;36mforward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    831\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/models/swin_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/models/swin_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_path1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_path2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/models/swin_transformer.py\u001b[0m in \u001b[0;36m_attn\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mattn_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         \u001b[0mattn_windows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_windows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# nW*B, window_size*window_size, C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0;31m# merge windows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/models/swin_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_rel_pos_bias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.80 GiB. GPU 0 has a total capacity of 15.89 GiB of which 3.63 GiB is free. Process 5540 has 12.25 GiB memory in use. Of the allocated memory 10.14 GiB is allocated by PyTorch, and 1.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 3.80 GiB. GPU 0 has a total capacity of 15.89 GiB of which 3.63 GiB is free. Process 5540 has 12.25 GiB memory in use. Of the allocated memory 10.14 GiB is allocated by PyTorch, and 1.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}