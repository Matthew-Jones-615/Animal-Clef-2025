{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#import timm\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torchvision.models.vision_transformer import vit_b_16, ViT_B_16_Weights\n",
    "from transformers import ViTMAEModel, AutoFeatureExtractor\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from animal_dataset import AnimalCLEFDataset  # now importable\n",
    "\n",
    "from transformers import ViTMAEForPreTraining, ViTMAEConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x113329c30>"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================\n",
    "# Configurations\n",
    "# =============================\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 0\n",
    "IMAGE_SIZE = 224\n",
    "LR = 1e-4\n",
    "\n",
    "epoch = 15\n",
    "EPOCHS_CLP = epoch\n",
    "EPOCHS_PLF = epoch\n",
    "\n",
    "LAMBDA = 0.3\n",
    "ALPHA_CL = 1#.2\n",
    "ALPHA_REC = 1#1.8\n",
    "\n",
    "FORCE_TRAIN_RESTART_CLP = True\n",
    "FORCE_TRAIN_RESTART_PLF = True\n",
    "\n",
    "# NEW: what percentile of your database‐scores to use as the open‐set cutoff\n",
    "OPENSET_PERCENTILE = 0.60  # try 0.50 (median), or 0.75, etc.\n",
    "\n",
    "#DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "CHECKPOINT_DIR = './checkpoints'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "root = '../animal-clef-2025_data'\n",
    "\n",
    "# reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Set device, Mac, GPU, or CPU\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# =============================\n",
    "# Dataset\n",
    "# =============================\n",
    "class AnimalCLEFDataset(Dataset):\n",
    "    def __init__(self, root, split=\"database\", transform=None):\n",
    "        self.root = root.rstrip('/')\n",
    "        meta = pd.read_csv(f\"{self.root}/metadata.csv\")\n",
    "        sel = meta[meta['path'].str.contains(f\"/{split}/\")].reset_index(drop=True)\n",
    "        if sel.empty:\n",
    "            raise ValueError(f\"No entries for split '{split}'\")\n",
    "\n",
    "        self.paths = sel['path'].tolist()\n",
    "        self.image_ids = sel['image_id'].tolist()\n",
    "\n",
    "        if split == 'database':\n",
    "            #  Use individual identity,  \n",
    "            ids = sel['identity'].astype(str)\n",
    "\n",
    "            #  Build mapping from identity string → label index\n",
    "            self.id2idx = {iid: i for i, iid in enumerate(sorted(ids.unique()))}\n",
    "\n",
    "            #  Map each sample's identity to its label\n",
    "            self.labels = ids.map(self.id2idx).tolist()\n",
    "\n",
    "            # Safety check\n",
    "            num_classes = len(self.id2idx)\n",
    "            assert all(0 <= label < num_classes for label in self.labels), \"Invalid labels found\"\n",
    "        else:\n",
    "            self.labels = [-1] * len(sel)\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img = Image.open(f\"{self.root}/{self.paths[i]}\").convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, self.labels[i]\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# MAE Encoder + Projection Head + Decoder\n",
    "# =============================\n",
    "class MAEFramework(nn.Module):\n",
    "    def __init__(self,\n",
    "                 model_name: str = \"facebook/vit-mae-base\",\n",
    "                 proj_dim:   int   = 256,\n",
    "                 mask_ratio: float = 0.75):\n",
    "        super().__init__()\n",
    "        # 1) encoder-only ViT‐MAE\n",
    "        self.encoder = ViTMAEModel.from_pretrained(model_name)\n",
    "        # 2) full MAE (encoder+decoder) for reconstruction\n",
    "        self.mae = ViTMAEForPreTraining.from_pretrained(model_name)\n",
    "        self.mae.config.mask_ratio = mask_ratio\n",
    "\n",
    "        # projection head on top of the [CLS]-token\n",
    "        self.embed_dim = self.encoder.config.hidden_size\n",
    "        self.proj_head = nn.Sequential(\n",
    "            nn.Linear(self.embed_dim, self.embed_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(self.embed_dim, proj_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values: torch.Tensor, return_feats: bool = False):\n",
    "        # ---  A) contrastive backbone  ---\n",
    "        enc_out = self.encoder(\n",
    "            pixel_values=pixel_values,\n",
    "            return_dict=True,\n",
    "            output_hidden_states=True        # ← request all layer outputs\n",
    "        )\n",
    "        cls_feat = enc_out.last_hidden_state[:, 0, :]   # (B, hidden_size)\n",
    "        proj     = self.proj_head(cls_feat)             # (B, proj_dim)\n",
    "\n",
    "        # ---  B) reconstruction head  ---\n",
    "        pre_out  = self.mae(pixel_values=pixel_values, return_dict=True)\n",
    "        rec_loss = pre_out.loss                         # scalar\n",
    "        rec      = self.mae.unpatchify(pre_out.logits)  # (B,3,H,W)\n",
    "\n",
    "        if return_feats:\n",
    "            # drop the patch‐embeddings and keep only the block outputs\n",
    "            feats = list(enc_out.hidden_states[1:])    # list of [B,seq_len,D]\n",
    "            return cls_feat, proj, rec_loss, rec, feats\n",
    "\n",
    "        return proj, rec_loss, rec\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"class MAEFramework(nn.Module):\n",
    "    def __init__(self,\n",
    "                 model_name: str = \"facebook/vit-mae-base\",\n",
    "                 proj_dim: int = 256,\n",
    "                 mask_ratio: float = 0.75):\n",
    "        super().__init__()\n",
    "        # 1) Load a Vision‐MAE model (encoder + decoder, pretrained weights available)\n",
    "        self.mae = ViTMAEForPreTraining.from_pretrained(model_name)\n",
    "        # you can override mask ratio if you like\n",
    "        self.mae.config.mask_ratio = mask_ratio\n",
    "\n",
    "        # 2) Projection head (same as before)\n",
    "        self.embed_dim = self.mae.config.hidden_size  # typically 768\n",
    "        self.proj_head = nn.Sequential(\n",
    "            nn.Linear(self.embed_dim, self.embed_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(self.embed_dim, proj_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values: torch.Tensor, return_feats: bool = False):\n",
    "        \n",
    "        #pixel_values: Tensor of shape (B,3,H,W), normalized to [0,1]\n",
    "        \n",
    "        # the model itself will sample a mask (mask_ratio),\n",
    "        # run encoder → decoder → unpatchify → compute reconstruction loss\n",
    "        outputs = self.mae(pixel_values=pixel_values, return_dict=True)\n",
    "\n",
    "        # HuggingFace returns:\n",
    "        #  - loss:   the MSE on masked patches averaged over patches & pixels\n",
    "        #  - predicted_pixel_values: the full (B,3,H,W) reconstruction\n",
    "        rec_loss = outputs.loss                # (scalar, averaged over batch)\n",
    "        rec      = outputs.reconstructed_pixel_values\n",
    "\n",
    "        # take CLS token from encoder (before decoder)\n",
    "        # note: HF stores the last hidden state of the encoder in .encoder_last_hidden_state\n",
    "        cls_feat = outputs.encoder_last_hidden_state[:, 0, :]\n",
    "        proj     = self.proj_head(cls_feat)\n",
    "\n",
    "        if return_feats:\n",
    "            # we don’t extract intermediate “layer_indices” feats here\n",
    "            return cls_feat, proj, rec_loss, rec, []\n",
    "        return proj, rec_loss, rec\n",
    "\"\"\";\n",
    "\n",
    "\"\"\"\n",
    "class MAEFramework(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int = 768,\n",
    "                 proj_dim: int = 256,\n",
    "                 decoder_dim: int = 256,\n",
    "                 layer_indices: list[int] = [3, 6, 9]):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        # 1) Backbone ViT\n",
    "        self.encoder = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "        self.encoder.head = nn.Identity()\n",
    "        self.layer_indices = set(layer_indices)\n",
    "\n",
    "        # 3) Projection head\n",
    "        self.proj_head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(embed_dim, proj_dim),\n",
    "        )\n",
    "        # 4) Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(embed_dim, decoder_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(decoder_dim, 3 * IMAGE_SIZE * IMAGE_SIZE),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, return_feats: bool = False):\n",
    "        B = x.size(0)\n",
    "        # patch embed\n",
    "        x_p = self.encoder.conv_proj(x)\n",
    "        x_p = x_p.flatten(2).transpose(1, 2)\n",
    "        cls_tok = self.encoder.class_token.expand(B, -1, -1)\n",
    "        tokens = torch.cat([cls_tok, x_p], dim=1)\n",
    "        tokens = tokens + self.encoder.encoder.pos_embedding\n",
    "\n",
    "        feats = []\n",
    "        for idx, block in enumerate(self.encoder.encoder.layers):\n",
    "            tokens = block(tokens)\n",
    "            if idx in self.layer_indices:\n",
    "                feats.append(tokens.clone())\n",
    "\n",
    "        cls_feat = self.encoder.encoder.ln(tokens[:, 0])\n",
    "        proj = self.proj_head(cls_feat)\n",
    "        rec = self.decoder(cls_feat).view(B, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "        rec_loss = F.mse_loss(rec, x, reduction='none').mean([1, 2, 3])\n",
    "\n",
    "        if return_feats:\n",
    "            return cls_feat, proj, rec_loss, rec, feats\n",
    "        return proj, rec_loss, rec\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Losses\n",
    "# =============================\n",
    "class SupConLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.5):\n",
    "        super().__init__()\n",
    "        self.temp = temperature\n",
    "\n",
    "    def forward(self, p1, p2, labels=None):\n",
    "        # NT-Xent instance-level contrastive loss\n",
    "        z = torch.cat([p1, p2], dim=0)\n",
    "        z = F.normalize(z, dim=1)\n",
    "        N = p1.size(0)\n",
    "        sim = torch.matmul(z, z.T) / self.temp\n",
    "        mask = torch.eye(2*N, device=sim.device).bool()\n",
    "        sim.masked_fill_(mask, -9e15)\n",
    "        idx = torch.arange(N, device=sim.device)\n",
    "        targets = torch.cat([idx + N, idx])\n",
    "        return F.cross_entropy(sim, targets)\n",
    "\n",
    "class ProtoLoss(nn.Module):\n",
    "    def forward(self, feats, prototypes, labels):\n",
    "        dist = torch.cdist(feats, prototypes)\n",
    "        return F.cross_entropy(-dist, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Utilities\n",
    "# =============================\n",
    "def compute_layer_distances(bef_feats, aft_feats, temperature=0.5):\n",
    "    total = []  # Initialize as a list to store distances per layer\n",
    "    for b, a in zip(bef_feats, aft_feats):\n",
    "        b_f, a_f = b.flatten(1), a.flatten(1)\n",
    "        eu = F.pairwise_distance(b_f, a_f)\n",
    "        cos = 1 - F.cosine_similarity(b_f, a_f, dim=1)\n",
    "        score = eu + temperature * cos  # this score is already (B,)\n",
    "        total.append(score)  # Add the current batch's score to the list\n",
    "    return torch.stack(total, dim=0).mean(dim=0)  # Stack and compute mean along dim=0\n",
    "\n",
    "def calculate_unknown_score(feat_bef, feat_aft, feat_vec, prototypes, lamda=1.0):\n",
    "    # 1. Compute multilayer feature distance (s_total)\n",
    "    s_total = compute_layer_distances(feat_bef, feat_aft, temperature=0.5)  # Can keep temp fixed or expose as param\n",
    "\n",
    "    # 2. Compute max prototype similarity (s_prototypes)\n",
    "    fv_n = F.normalize(feat_vec, dim=1)  # (B, D)\n",
    "    p_n = F.normalize(prototypes, dim=1)  # (C, D)\n",
    "    sim = torch.matmul(fv_n, p_n.T)  # (B, C)\n",
    "    s_proto, _ = sim.max(dim=1)  # (B,)\n",
    "\n",
    "    # 3. Final score using lambda\n",
    "    score = s_proto - lamda * s_total\n",
    "\n",
    "    return score  # (B,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_CLP(model, loader, epochs, lr, ckpt, alpha_cl=1.0, alpha_rec=1.0, force_restart=False):\n",
    "    \"\"\"\n",
    "    Contrastive‐plus‐reconstruction pre-training, but with\n",
    "    supervised CE on the CLS token instead of instance‐level SupCon.\n",
    "    \"\"\"\n",
    "    cl_hist, rec_hist, tot_hist = [], [], []\n",
    "    path = os.path.join(CHECKPOINT_DIR, ckpt)\n",
    "\n",
    "    # --- NEW: figure out how many classes we have in the 'database' split\n",
    "    num_classes = len(loader.dataset.id2idx)\n",
    "\n",
    "    # --- NEW: add a tiny classification head on CLS\n",
    "    clf_head = nn.Linear(model.embed_dim, num_classes).to(DEVICE)\n",
    "\n",
    "    # --- NEW: optimizer now includes both the ViT parameters AND the new head\n",
    "    opt = torch.optim.Adam(\n",
    "        list(model.parameters()) + list(clf_head.parameters()),\n",
    "        lr=lr\n",
    "    )\n",
    "    start = 0\n",
    "\n",
    "    if os.path.exists(path) and force_restart:\n",
    "        print(f\"[train_CLP] Restarting from scratch (deleting {path})\")\n",
    "        os.remove(path)\n",
    "\n",
    "    # --- load checkpoint as before (we assume it saved only model & opt & ep) ---\n",
    "    if os.path.exists(path):\n",
    "        ck     = torch.load(path, map_location=DEVICE)\n",
    "        raw_sd = ck['model']\n",
    "        sd     = {k.replace(\"module.\", \"\"): v for k,v in raw_sd.items()}\n",
    "        model.load_state_dict(sd)\n",
    "        clf_head.load_state_dict(ck['clf_head'])          # NEW: restore classifier\n",
    "        opt.load_state_dict(ck['opt'])\n",
    "        start = ck['ep'] + 1\n",
    "\n",
    "    model.to(DEVICE)\n",
    "    clf_head.to(DEVICE)\n",
    "\n",
    "    for ep in range(start, epochs):\n",
    "        model.train()\n",
    "        total_ce, total_rec, total = 0.0, 0.0, 0.0\n",
    "\n",
    "        for x, y in tqdm(loader, desc=f\"CLP Ep{ep+1}\"):\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "            # two‐view reconstructions for the MAE\n",
    "            x1 = x + 0.05*torch.randn_like(x)\n",
    "            x2 = x + 0.05*torch.randn_like(x)\n",
    "\n",
    "            # forward‐pass with features\n",
    "            cls_feat, _, r1_loss, _, _ = model(x1, return_feats=True)\n",
    "            _,       _, r2_loss, _, _ = model(x2, return_feats=True)\n",
    "\n",
    "            # --- NEW: supervised cross‐entropy on the CLS embedding\n",
    "            logits = clf_head(cls_feat)               # (B, num_classes)\n",
    "            l_ce   = F.cross_entropy(logits, y)       # standard CE\n",
    "\n",
    "            # reconstruction loss (unchanged)\n",
    "            l_rec  = r1_loss.mean() + r2_loss.mean()\n",
    "\n",
    "            loss = alpha_cl * l_ce + alpha_rec * l_rec\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            total     += loss.item()\n",
    "            total_ce  += l_ce.item()\n",
    "            total_rec += l_rec.item()\n",
    "\n",
    "        # --- save checkpoint (model, classifier, optimizer, epoch)\n",
    "        torch.save({\n",
    "            'model':     model.module.state_dict() if isinstance(model, nn.DataParallel)\n",
    "                                               else model.state_dict(),\n",
    "            'clf_head': clf_head.state_dict(),         # NEW\n",
    "            'opt':       opt.state_dict(),\n",
    "            'ep':        ep\n",
    "        }, path)\n",
    "\n",
    "        avg_ce  = total_ce  / len(loader)\n",
    "        avg_rec = total_rec / len(loader)\n",
    "        avg_tot = total   / len(loader)\n",
    "\n",
    "        cl_hist.append(avg_ce)\n",
    "        rec_hist.append(avg_rec)\n",
    "        tot_hist.append(avg_tot)\n",
    "\n",
    "        print(f\"CLP Epoch {ep+1}: total={avg_tot:.4f}, CE={avg_ce:.4f}, rec={avg_rec:.4f}\")\n",
    "\n",
    "    return model, cl_hist, rec_hist, tot_hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def train_CLP(model, loader, epochs, lr, ckpt, alpha_cl=1.0, alpha_rec=1.0):\n",
    "    \n",
    "    path = os.path.join(CHECKPOINT_DIR, ckpt)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    start = 0\n",
    "\n",
    "    # --- load checkpoint if it exists, stripping \"module.\" if necessary\n",
    "    if os.path.exists(path):\n",
    "        ck = torch.load(path, map_location=DEVICE)\n",
    "        raw_sd = ck['model']\n",
    "        # strip DataParallel \"module.\" prefix\n",
    "        sd = {k.replace(\"module.\", \"\"): v for k, v in raw_sd.items()}\n",
    "        model.load_state_dict(sd)\n",
    "        opt.load_state_dict(ck['opt'])\n",
    "        start = ck['ep'] + 1\n",
    "    # ---------------------------------------------------------------\n",
    "\n",
    "    scl = SupConLoss()\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    for ep in range(start, epochs):\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "        for x, y in tqdm(loader, desc=f\"CLP Ep{ep+1}\"):\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            x1 = x + 0.05 * torch.randn_like(x)\n",
    "            x2 = x + 0.05 * torch.randn_like(x)\n",
    "            f1, p1, r1_loss, r1, _ = model(x1, return_feats=True)\n",
    "            f2, p2, r2_loss, r2, _ = model(x2, return_feats=True)\n",
    "            l_cl  = scl(p1, p2)\n",
    "            l_rec = r1_loss.mean() + r2_loss.mean()\n",
    "            loss  = alpha_cl * l_cl + alpha_rec * l_rec\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total += loss.item()\n",
    "\n",
    "        torch.save({\n",
    "            'model': model.state_dict() if not isinstance(model, nn.DataParallel)\n",
    "                              else model.module.state_dict(),\n",
    "            'opt':   opt.state_dict(),\n",
    "            'ep':    ep\n",
    "        }, path)\n",
    "        print(f\"CLP Epoch {ep+1}: {total/len(loader):.4f}\")\n",
    "        \"\"\";\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def train_PLF(model, loader, epochs, lr, ckpt, num_classes, force_restart = False):\n",
    "    path = os.path.join(CHECKPOINT_DIR, ckpt)\n",
    "    \n",
    "        # if asked, delete the old checkpoint\n",
    "    if os.path.exists(path) and force_restart:\n",
    "        print(f\"Checkpoint {path} exists. Restarting PLF training.\")\n",
    "        os.remove(path)\n",
    "    \n",
    "    # unwrap if DataParallel\n",
    "    base_model = model.module if isinstance(model, nn.DataParallel) else model\n",
    "    embed_dim   = base_model.embed_dim\n",
    "\n",
    "    proto_tensor = torch.randn(num_classes, embed_dim, device=DEVICE)\n",
    "    proto = nn.Parameter(proto_tensor, requires_grad=True)\n",
    "    opt   = torch.optim.Adam(list(model.parameters()) + [proto], lr=lr)\n",
    "    start = 0\n",
    "\n",
    "    # --- load checkpoint if it exists, stripping \"module.\" if necessary\n",
    "    if os.path.exists(path):\n",
    "        ck = torch.load(path, map_location=DEVICE)\n",
    "        raw_sd = ck['model']\n",
    "        sd = {k.replace(\"module.\", \"\"): v for k, v in raw_sd.items()}\n",
    "        model.load_state_dict(sd)\n",
    "        proto.data = ck['proto']\n",
    "        opt.load_state_dict(ck['opt'])\n",
    "        start = ck['ep'] + 1\n",
    "    # ---------------------------------------------------------------\n",
    "\n",
    "    ploss = ProtoLoss()\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    for ep in range(start, epochs):\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "        for x, y in tqdm(loader, desc=f\"PLF Ep{ep+1}\"):\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            feats, _, _, _, _ = model(x, return_feats=True)\n",
    "            loss = ploss(feats, proto, y)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total += loss.item()\n",
    "\n",
    "        torch.save({\n",
    "            'model': model.state_dict() if not isinstance(model, nn.DataParallel)\n",
    "                              else model.module.state_dict(),\n",
    "            'proto': proto.data,\n",
    "            'opt':   opt.state_dict(),\n",
    "            'ep':    ep\n",
    "        }, path)\n",
    "        print(f\"PLF Epoch {ep+1}: {total/len(loader):.4f}\")\n",
    "\n",
    "    return proto\n",
    "\"\"\";\n",
    "\n",
    "# =============================\n",
    "# Prototype‐only training\n",
    "# =============================\n",
    "def train_PLF(model, loader, epochs, lr, ckpt, num_classes, force_restart=False, unfrozen_encoder=False):\n",
    "    \"\"\"\n",
    "    Train only the prototypes and the proj_head parameters.\n",
    "    \"\"\"\n",
    "    path = os.path.join(CHECKPOINT_DIR, ckpt)\n",
    "\n",
    "    # 1) create fresh prototypes\n",
    "    proto = nn.Parameter(torch.randn(num_classes, model.embed_dim, device=DEVICE))\n",
    "    \n",
    "    # 2) build optimizer params: always proj_head + proto,\n",
    "    #    and optionally any encoder layers you’ve unfrozen\n",
    "    params = list(model.proj_head.parameters()) + [proto]\n",
    "    if unfrozen_encoder:\n",
    "        \n",
    "        # include only those encoder params that we explicitly un-froze\n",
    "        params += [p for p in model.encoder.parameters() if p.requires_grad]\n",
    "    opt = torch.optim.Adam(params, lr=lr)\n",
    "\n",
    "    \"\"\"\n",
    "    # 2) optimizer over just proj_head + proto\n",
    "    opt = torch.optim.Adam(\n",
    "        list(model.proj_head.parameters()) + [proto],\n",
    "        lr=lr\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    start = 0\n",
    "    if os.path.exists(path) and force_restart:\n",
    "        os.remove(path)\n",
    "    if os.path.exists(path):\n",
    "        ck = torch.load(path, map_location=DEVICE)\n",
    "        proto.data = ck['proto']\n",
    "        model.proj_head.load_state_dict(ck['proj_head'])\n",
    "        opt.load_state_dict(ck['opt'])\n",
    "        start = ck['ep'] + 1\n",
    "\n",
    "    ploss = ProtoLoss()\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    for ep in range(start, epochs):\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "        for x, y in tqdm(loader, desc=f\"PLF Ep{ep+1}\"):\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            # forward through *frozen* encoder + proj_head\n",
    "            feats, _, _, _, _ = model(x, return_feats=True)\n",
    "            loss = ploss(feats, proto, y)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total += loss.item()\n",
    "\n",
    "        # save checkpoint (prototypes, proj_head, optimizer, epoch)\n",
    "        torch.save({\n",
    "            'proto':     proto.data,\n",
    "            'proj_head': model.proj_head.state_dict(),\n",
    "            'opt':       opt.state_dict(),\n",
    "            'ep':        ep\n",
    "        }, path)\n",
    "\n",
    "        print(f\"PLF Epoch {ep+1}: {total/len(loader):.4f}\")\n",
    "\n",
    "    return proto\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# =============================\n",
    "# Training: CLP\n",
    "# =============================\n",
    "def train_CLP(model, loader, epochs, lr, ckpt, alpha_cl=1.0, alpha_rec=1.0):\n",
    "    path = os.path.join(CHECKPOINT_DIR, ckpt)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    start = 0\n",
    "    if os.path.exists(path):\n",
    "        ck = torch.load(path)\n",
    "        model.load_state_dict(ck['model'])\n",
    "        opt.load_state_dict(ck['opt'])\n",
    "        start = ck['ep'] + 1\n",
    "    scl = SupConLoss()\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    for ep in range(start, epochs):\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "        for x, y in tqdm(loader, desc=f\"CLP Ep{ep+1}\"):\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            x1 = x + 0.05 * torch.randn_like(x)\n",
    "            x2 = x + 0.05 * torch.randn_like(x)\n",
    "            f1, p1, r1_loss, r1, _ = model(x1, return_feats=True)\n",
    "            f2, p2, r2_loss, r2, _ = model(x2, return_feats=True)\n",
    "            l_cl = scl(p1, p2)\n",
    "            l_rec = r1_loss.mean() + r2_loss.mean()\n",
    "            loss = alpha_cl * l_cl + alpha_rec * l_rec\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total += loss.item()\n",
    "        torch.save({'model': model.state_dict(), 'opt': opt.state_dict(), 'ep': ep}, path)\n",
    "        print(f\"CLP Epoch {ep+1}: {total/len(loader):.4f}\")\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Training: PLF\n",
    "# =============================\n",
    "def train_PLF(model, loader, epochs, lr, ckpt, num_classes, encoder_pre):\n",
    "    path = os.path.join(CHECKPOINT_DIR, ckpt)\n",
    "    # unwrap if DataParallel\n",
    "    base_model = model.module if isinstance(model, nn.DataParallel) else model\n",
    "    embed_dim = base_model.embed_dim\n",
    "    proto_tensor = torch.randn(num_classes, embed_dim, device=DEVICE)\n",
    "    proto = nn.Parameter(proto_tensor, requires_grad=True)\n",
    "    opt = torch.optim.Adam(list(model.parameters()) + [proto], lr=lr)\n",
    "    start = 0\n",
    "    if os.path.exists(path):\n",
    "        ck = torch.load(path)\n",
    "        model.load_state_dict(ck['model'])\n",
    "        proto.data = ck['proto']\n",
    "        opt.load_state_dict(ck['opt'])\n",
    "        start = ck['ep'] + 1\n",
    "    ploss = ProtoLoss()\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    for ep in range(start, epochs):\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "        for x, y in tqdm(loader, desc=f\"PLF Ep{ep+1}\"):\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            feats, _, _, _, _ = model(x, return_feats=True)\n",
    "            loss = ploss(feats, proto, y)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total += loss.item()\n",
    "        torch.save({'model': model.state_dict(), 'proto': proto.data, 'opt': opt.state_dict(), 'ep': ep}, path)\n",
    "        print(f\"PLF Epoch {ep+1}: {total/len(loader):.4f}\")\n",
    "    return proto\n",
    "\"\"\";\n",
    "    \n",
    "# =============================\n",
    "# Inference\n",
    "# =============================\n",
    "\n",
    "@torch.no_grad()\n",
    "def inference(enc_pre, model, proto, loader, threshold, lamda=1.0):\n",
    "    enc_pre.eval()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "\n",
    "    for x, _ in tqdm(loader, desc=\"Infer\"):\n",
    "        x = x.to(DEVICE)\n",
    "        enc_pre = enc_pre.to(DEVICE)\n",
    "        model = model.to(DEVICE)\n",
    "\n",
    "        _, _, _, _, bef_feats = enc_pre(x, return_feats=True)\n",
    "        feat, _, _, _, aft_feats = model(x, return_feats=True)\n",
    "\n",
    "        # 1. Calculate unknown score\n",
    "        scores = calculate_unknown_score(bef_feats, aft_feats, feat, proto, lamda = lamda)  # shape: (B,)\n",
    "\n",
    "        # 2. Predict the most similar known class\n",
    "        idx = torch.argmax(torch.matmul(F.normalize(feat, dim=1), F.normalize(proto, dim=1).T), dim=1)  # (B,)\n",
    "\n",
    "        # 3. Threshold to filter unknowns\n",
    "        known = scores > threshold\n",
    "        pred = idx.clone()\n",
    "        pred[~known] = -1  # mark unknowns\n",
    "\n",
    "        preds.append(pred.cpu())\n",
    "\n",
    "    return torch.cat(preds, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Submission\n",
    "# =============================\n",
    "def generate_submission(root, preds, db_ds):\n",
    "    sub = pd.read_csv(f\"{root}/sample_submission.csv\")\n",
    "    meta = pd.read_csv(f\"{root}/metadata.csv\")\n",
    "    q = meta[meta['path'].str.contains('/query/')].reset_index(drop=True)\n",
    "    q['pred_idx'] = preds.numpy()\n",
    "\n",
    "    # Step 1: Print raw predicted indices\n",
    "    print(\"\\nRaw predicted indices (pred_idx):\")\n",
    "    print(q[['image_id', 'pred_idx']].head(10))\n",
    "\n",
    "    # Step 2: Index to identity mapping\n",
    "    idx2id = {v: k for k, v in db_ds.id2idx.items()}\n",
    "    q['prediction'] = q['pred_idx'].apply(lambda i: 'new_individual' if i < 0 else idx2id.get(int(i), f\"unknown_{i}\"))\n",
    "\n",
    "    # Step 3: Print mapped predictions\n",
    "    print(\"\\nMapped predictions (after idx2id):\")\n",
    "    print(q[['image_id', 'prediction']].head(10))\n",
    "\n",
    "    # drop the original metadata identity so we don't end up with two columns\n",
    "    q = q.drop(columns=['identity'])\n",
    "\n",
    "    # rename 'prediction' → 'identity' so the output column is called identity\n",
    "    q = q.rename(columns={'prediction':'identity'})\n",
    "\n",
    "    # now merge on image_id and keep the 'identity' column\n",
    "    out = sub[['image_id']].merge(q[['image_id','identity']], on='image_id')\n",
    "\n",
    "    #out = sub[['image_id']].merge(q[['image_id', 'identity']], on='image_id')\n",
    "    #out = sub[['image_id']].merge(q[['image_id','prediction']], on='image_id')\n",
    "\n",
    "\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    save_path = f'submission_{timestamp}.csv'\n",
    "    out.to_csv(save_path, index=False)\n",
    "\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthewjones2/anaconda3/envs/574_animal_clef_env/lib/python3.11/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train_CLP] Restarting from scratch (deleting ./checkpoints/clp.pth)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLP Ep1: 100%|██████████| 409/409 [07:12<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLP Epoch 1: total=5.8306, CE=5.5661, rec=0.2644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLP Ep2:  68%|██████▊   | 277/409 [05:01<02:30,  1.14s/it]"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Main Workflow\n",
    "# =============================\n",
    "\"\"\"\n",
    "def main():\n",
    "    #root = '/kaggle/input/animal-clef-2025'\n",
    "    tf = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    #idx2id = {v: k for k, v in db_ds.id2idx.items()}\n",
    "\n",
    "    db_ds = AnimalCLEFDataset(root, 'database', transform=tf)\n",
    "    \n",
    "    #print(\"Example metadata path[0]:\", db_ds.paths[0])\n",
    "    #print(\"Looking for file at:\", os.path.join(db_ds.root, db_ds.paths[0]))\n",
    "    #print(\"Exists on disk?\", os.path.exists(os.path.join(db_ds.root, db_ds.paths[0])))\n",
    "    \n",
    "    db_loader = DataLoader(db_ds, batch_size = BATCH_SIZE, shuffle=True,\n",
    "                           num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    query_ds = AnimalCLEFDataset(root, 'query', transform=tf)\n",
    "    q_loader = DataLoader(query_ds, batch_size = BATCH_SIZE, shuffle=False,\n",
    "                           num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "    clp_model = MAEFramework()\n",
    "    plf_model = MAEFramework()\n",
    "\n",
    "    # pick your device\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        DEVICE = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        DEVICE = torch.device(\"cuda\")\n",
    "    else:\n",
    "        DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "    # decide which GPUs to wrap\n",
    "    cuda_gpus = torch.cuda.device_count()\n",
    "    if DEVICE.type == \"cuda\" and cuda_gpus > 1:\n",
    "        device_ids = list(range(cuda_gpus))  # e.g. [0,1,…]\n",
    "        clp_model = nn.DataParallel(clp_model,  device_ids=device_ids).to(DEVICE)\n",
    "        plf_model = nn.DataParallel(plf_model,  device_ids=device_ids).to(DEVICE)\n",
    "    else:\n",
    "        # single‐device: MPS, single CUDA, or CPU\n",
    "        clp_model = clp_model.to(DEVICE)\n",
    "        plf_model = plf_model.to(DEVICE)\n",
    "\n",
    "    # now training and inference will work on either MPS or CUDA\n",
    "    #train_CLP(clp_model, db_loader, EPOCHS_CLP, LR,  'clp.pth', alpha_cl = ALPHA_CL, alpha_rec = ALPHA_REC)\n",
    "    \n",
    "    clp_model, cl_hist, rec_hist, tot_hist = train_CLP(\n",
    "        clp_model, db_loader, EPOCHS_CLP, LR, 'clp.pth',\n",
    "        alpha_cl=ALPHA_CL, alpha_rec=ALPHA_REC, force_restart=FORCE_TRAIN_RESTART_CLP)\n",
    "    \n",
    "    epochs = list(range(1, len(cl_hist)+1))\n",
    "    plt.plot(epochs, cl_hist, label='Contrastive Loss')\n",
    "    plt.plot(epochs, rec_hist, label='Reconstruction Loss')\n",
    "    plt.plot(epochs, tot_hist, label='Total Loss')\n",
    "\n",
    "    plt.title(f'CLP Losses (α_cl={ALPHA_CL}, α_rec={ALPHA_REC}, λ={LAMBDA}, batch={BATCH_SIZE})')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    prototype = train_PLF(plf_model, db_loader, EPOCHS_PLF, LR,\n",
    "                          'plf.pth', len(db_ds.id2idx),  force_restart = FORCE_TRAIN_RESTART_PLF)\n",
    "    \n",
    "\n",
    "    \n",
    "    print(\"Prototype shape:\", prototype.shape)\n",
    "    print(\"Number of unique classes:\", len(db_ds.id2idx))\n",
    "\n",
    "    # Plot the score distribution before estimating threshold\n",
    "    #plot_score_distribution(clp_model, plf_model, prototype, db_loader)\n",
    "\n",
    "    # Estimate threshold from database distribution\n",
    "    dists = []\n",
    "    with torch.no_grad():\n",
    "        for x, _ in DataLoader(db_ds, batch_size = BATCH_SIZE, shuffle=False,\n",
    "                               num_workers=NUM_WORKERS, pin_memory=True):\n",
    "            x = x.to(DEVICE)\n",
    "            feat, _, _, _, aft_feats = plf_model(x, return_feats=True)\n",
    "            _, _, _, _, bef_feats = clp_model(x, return_feats=True)\n",
    "            scores = calculate_unknown_score(bef_feats, aft_feats, feat, prototype, lamda = LAMBDA)\n",
    "            dists.extend(scores.cpu().tolist())\n",
    "\n",
    "\n",
    "    threshold = torch.quantile(torch.tensor(dists), 0.95).item()\n",
    "\n",
    "\n",
    "    # Inference on query set\n",
    "    with torch.no_grad():\n",
    "        #lamda = 0.2 # or any value you want to test\n",
    "        preds = inference(clp_model, plf_model, prototype, q_loader, threshold, lamda = LAMBDA)\n",
    "\n",
    "\n",
    "    print(\"Saving submission…\")\n",
    "    generate_submission(root, preds, db_ds)\n",
    "\"\"\";\n",
    "def main():\n",
    "    # transforms & datasets\n",
    "    #tf = transforms.Compose([\n",
    "    #    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    #    transforms.ToTensor()\n",
    "    #])\n",
    "\n",
    "    feat_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/vit-mae-base\")\n",
    "    mean = feat_extractor.image_mean\n",
    "    std  = feat_extractor.image_std\n",
    "\n",
    "    tf = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std),\n",
    "    ])\n",
    "\n",
    "    db_ds    = AnimalCLEFDataset(root, 'database', transform=tf)\n",
    "    query_ds = AnimalCLEFDataset(root, 'query',    transform=tf)\n",
    "    \n",
    "    pin = (DEVICE.type == \"cuda\")\n",
    "\n",
    "    db_loader = DataLoader(db_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                           num_workers=NUM_WORKERS, pin_memory=pin)\n",
    "    q_loader  = DataLoader(query_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                           num_workers=NUM_WORKERS, pin_memory=pin)\n",
    "\n",
    "    # --- 1) Train CLP as before ---\n",
    "    clp_model = MAEFramework().to(DEVICE)\n",
    "    if DEVICE.type == \"cuda\" and torch.cuda.device_count() > 1:\n",
    "        clp_model = nn.DataParallel(clp_model)\n",
    "    clp_model, cl_hist, rec_hist, tot_hist = train_CLP(\n",
    "        clp_model, db_loader, EPOCHS_CLP, LR, 'clp.pth',\n",
    "        alpha_cl=ALPHA_CL, alpha_rec=ALPHA_REC,\n",
    "        force_restart=FORCE_TRAIN_RESTART_CLP\n",
    "    )\n",
    "\n",
    "    # plot losses\n",
    "    epochs = list(range(1, len(cl_hist)+1))\n",
    "    plt.plot(epochs, cl_hist, label='Contrastive')\n",
    "    plt.plot(epochs, rec_hist,  label='Reconstruction')\n",
    "    plt.plot(epochs, tot_hist,  label='Total')\n",
    "    #plt.title(f\"CLP Losses (α_cl={ALPHA_CL}, α_rec={ALPHA_REC})\")\n",
    "    \n",
    "    plt.title(\n",
    "        f\"Contrastive and Reconstruction Losses (α_cl={ALPHA_CL}, α_rec={ALPHA_REC})\\n\"\n",
    "        f\"λ={LAMBDA}, bs={BATCH_SIZE}, LR={LR}\\n\"\n",
    "        f\"{int(OPENSET_PERCENTILE*100)}%-quantile cutoff={threshold:.4f}\"\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend(); plt.show()\n",
    "\n",
    "    # --- 2) Initialize PLF from CLP encoder & freeze everything except proj_head & proto ---\n",
    "    #plf_model = MAEFramework().to(DEVICE)\n",
    "    \n",
    "    # --- 2) Initialize PLF from CLP encoder & freeze everything except proj_head & proto ---\n",
    "    plf_model = MAEFramework().to(DEVICE)\n",
    "\n",
    "    # copy encoder, head, decoder weights\n",
    "    if isinstance(clp_model, nn.DataParallel):\n",
    "        enc_sd  = clp_model.module.encoder.state_dict()\n",
    "        head_sd = clp_model.module.proj_head.state_dict()\n",
    "        dec_sd  = clp_model.module.mae.state_dict()\n",
    "    else:\n",
    "        enc_sd  = clp_model.encoder.state_dict()\n",
    "        head_sd = clp_model.proj_head.state_dict()\n",
    "        dec_sd  = clp_model.mae.state_dict()\n",
    "\n",
    "    plf_model.encoder.load_state_dict(enc_sd)\n",
    "    plf_model.proj_head.load_state_dict(head_sd)\n",
    "    plf_model.mae.load_state_dict(dec_sd, strict=False)\n",
    "\n",
    "    # only need one decoder‐freeze pass now\n",
    "    #for p in plf_model.mae.parameters():\n",
    "    #    p.requires_grad = False\n",
    "\n",
    "    # 2) freeze entire MAE backbone (encoder + decoder)\n",
    "    for p in plf_model.encoder.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in plf_model.mae.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # NEW: unfreeze the *last* 2 ViT blocks so they can adapt to class prototypes\n",
    "    for blk in plf_model.encoder.encoder.layer[-2:]:\n",
    "        for p in blk.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    # proj_head remains trainable by default\n",
    "\n",
    "    if DEVICE.type == \"cuda\" and torch.cuda.device_count() > 1:\n",
    "        plf_model = nn.DataParallel(plf_model)\n",
    "\n",
    "    \"\"\"\n",
    "    # --- 3) Train PLF (only prototypes + proj_head) ---\n",
    "    prototype = train_PLF(\n",
    "        plf_model, db_loader, EPOCHS_PLF, LR, 'plf.pth',\n",
    "        len(db_ds.id2idx),\n",
    "        force_restart=FORCE_TRAIN_RESTART_PLF\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 3) Train PLF (prototypes + proj_head + last 2 encoder blocks) ---\n",
    "    # Make sure train_PLF’s optimizer picks up the newly-unfrozen layers:\n",
    "    prototype = train_PLF(\n",
    "        plf_model, db_loader, EPOCHS_PLF, LR, 'plf.pth',\n",
    "        len(db_ds.id2idx),\n",
    "        force_restart=FORCE_TRAIN_RESTART_PLF,\n",
    "        unfrozen_encoder=True  # ← you can add a flag so train_PLF adds encoder params\n",
    "    )\n",
    "\n",
    "    print(\"Prototype shape:\", prototype.shape)\n",
    "\n",
    "    # --- 4) Estimate threshold ---\n",
    "    dists = []\n",
    "    with torch.no_grad():\n",
    "        for x, _ in DataLoader(db_ds, batch_size=BATCH_SIZE,\n",
    "                               shuffle=False, num_workers=NUM_WORKERS, pin_memory=pin):\n",
    "            x = x.to(DEVICE)\n",
    "            _, _, _, _, bef_feats = clp_model(x, return_feats=True)\n",
    "            _, _, _, _, aft_feats = plf_model(x, return_feats=True)\n",
    "            feat, _, _, _, _      = plf_model(x, return_feats=True)\n",
    "            scores = calculate_unknown_score(bef_feats, aft_feats, feat, prototype, lamda=LAMBDA)\n",
    "            #print(\"min, max, 95%-thresh of database scores:\", \n",
    "            #min(dists), max(dists), threshold)\n",
    "            dists.extend(scores.cpu().tolist())\n",
    "    threshold = torch.quantile(torch.tensor(dists), OPENSET_PERCENTILE).item()\n",
    "\n",
    "    #print(f\"DB scores → min {min(dists):.4f}, max {max(dists):.4f}, 95%-quantile {threshold:.4f}, frac≥thresh {(np.array(dists)>threshold).mean():.2%}\")\n",
    "    print(\n",
    "        f\"DB scores → \"\n",
    "        f\"min {min(dists):.4f}, \"\n",
    "        f\"max {max(dists):.4f}, \"\n",
    "        f\"{int(OPENSET_PERCENTILE*100)}%-quantile {threshold:.4f}, \"\n",
    "        f\"frac≥thresh {(np.array(dists) > threshold).mean():.2%}\"\n",
    "    )\n",
    "\n",
    "\n",
    "    # --- 5) Inference + submission ---\n",
    "    with torch.no_grad():\n",
    "        preds = inference(clp_model, plf_model, prototype, q_loader, threshold, lamda=LAMBDA)\n",
    "    print(\"Saving submission…\")\n",
    "    generate_submission(root, preds, db_ds)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds = inference(clp_model, plf_model, prototype, q_loader, threshold, lamda=lamda)\n",
    "#print(\"Saving submission…\")\n",
    "#generate_submission(root, preds, db_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTMAEForPreTraining, ViTMAEConfig"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11223220,
     "sourceId": 91451,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "574_animal_clef_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
