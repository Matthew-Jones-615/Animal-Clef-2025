{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91451,"databundleVersionId":11223220,"sourceType":"competition"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport pandas as pd\nfrom torchvision.models.vision_transformer import vit_b_16, ViT_B_16_Weights\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\n# =============================\n# Configurations\n# =============================\nBATCH_SIZE = 32\nNUM_WORKERS = 4\nIMAGE_SIZE = 224\nEPOCHS_CLP = 1\nEPOCHS_PLF = 1\nLR = 1e-4\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nCHECKPOINT_DIR = './checkpoints'\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\n\n# reproducibility\ntorch.manual_seed(42)\n\n# =============================\n# Dataset\n# =============================\nclass AnimalCLEFDataset(Dataset):\n    def __init__(self, root, split=\"database\", transform=None):\n        self.root = root.rstrip('/')\n        meta = pd.read_csv(f\"{self.root}/metadata.csv\")\n        sel = meta[meta['path'].str.contains(f\"/{split}/\")].reset_index(drop=True)\n        if sel.empty:\n            raise ValueError(f\"No entries for split '{split}'\")\n\n        self.paths = sel['path'].tolist()\n        self.image_ids = sel['image_id'].tolist()\n\n        if split == 'database':\n            #  Use individual identity,  \n            ids = sel['identity'].astype(str)\n\n            #  Build mapping from identity string â†’ label index\n            self.id2idx = {iid: i for i, iid in enumerate(sorted(ids.unique()))}\n\n            #  Map each sample's identity to its label\n            self.labels = ids.map(self.id2idx).tolist()\n\n            # Safety check\n            num_classes = len(self.id2idx)\n            assert all(0 <= label < num_classes for label in self.labels), \"Invalid labels found\"\n        else:\n            self.labels = [-1] * len(sel)\n\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.paths)\n\n    def __getitem__(self, i):\n        img = Image.open(f\"{self.root}/{self.paths[i]}\").convert('RGB')\n        if self.transform:\n            img = self.transform(img)\n        return img, self.labels[i]\n\n# =============================\n# MAE Encoder + Projection Head + Decoder\n# =============================\nclass MAEFramework(nn.Module):\n    def __init__(self,\n                 embed_dim: int = 768,\n                 proj_dim: int = 256,\n                 decoder_dim: int = 256,\n                 layer_indices: list[int] = [3, 6, 9]):\n        super().__init__()\n        self.embed_dim = embed_dim\n        # 1) Backbone ViT\n        self.encoder = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n        self.encoder.head = nn.Identity()\n        self.layer_indices = set(layer_indices)\n\n        # 3) Projection head\n        self.proj_head = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(embed_dim, proj_dim),\n        )\n        # 4) Decoder\n        self.decoder = nn.Sequential(\n            nn.Linear(embed_dim, decoder_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(decoder_dim, 3 * IMAGE_SIZE * IMAGE_SIZE),\n        )\n\n    def forward(self, x: torch.Tensor, return_feats: bool = False):\n        B = x.size(0)\n        # patch embed\n        x_p = self.encoder.conv_proj(x)\n        x_p = x_p.flatten(2).transpose(1, 2)\n        cls_tok = self.encoder.class_token.expand(B, -1, -1)\n        tokens = torch.cat([cls_tok, x_p], dim=1)\n        tokens = tokens + self.encoder.encoder.pos_embedding\n\n        feats = []\n        for idx, block in enumerate(self.encoder.encoder.layers):\n            tokens = block(tokens)\n            if idx in self.layer_indices:\n                feats.append(tokens.clone())\n\n        cls_feat = self.encoder.encoder.ln(tokens[:, 0])\n        proj = self.proj_head(cls_feat)\n        rec = self.decoder(cls_feat).view(B, 3, IMAGE_SIZE, IMAGE_SIZE)\n        rec_loss = F.mse_loss(rec, x, reduction='none').mean([1, 2, 3])\n\n        if return_feats:\n            return cls_feat, proj, rec_loss, rec, feats\n        return proj, rec_loss, rec\n\n# =============================\n# Losses\n# =============================\nclass SupConLoss(nn.Module):\n    def __init__(self, temperature=0.5):\n        super().__init__()\n        self.temp = temperature\n\n    def forward(self, p1, p2, labels=None):\n        # NT-Xent instance-level contrastive loss\n        z = torch.cat([p1, p2], dim=0)\n        z = F.normalize(z, dim=1)\n        N = p1.size(0)\n        sim = torch.matmul(z, z.T) / self.temp\n        mask = torch.eye(2*N, device=sim.device).bool()\n        sim.masked_fill_(mask, -9e15)\n        idx = torch.arange(N, device=sim.device)\n        targets = torch.cat([idx + N, idx])\n        return F.cross_entropy(sim, targets)\n\nclass ProtoLoss(nn.Module):\n    def forward(self, feats, prototypes, labels):\n        dist = torch.cdist(feats, prototypes)\n        return F.cross_entropy(-dist, labels)\n\n# =============================\n# Utilities\n# =============================\ndef compute_layer_distances(bef_feats, aft_feats, temperature=0.5):\n    total = []  # Initialize as a list to store distances per layer\n    for b, a in zip(bef_feats, aft_feats):\n        b_f, a_f = b.flatten(1), a.flatten(1)\n        eu = F.pairwise_distance(b_f, a_f)\n        cos = 1 - F.cosine_similarity(b_f, a_f, dim=1)\n        score = eu + temperature * cos  # this score is already (B,)\n        total.append(score)  # Add the current batch's score to the list\n    return torch.stack(total, dim=0).mean(dim=0)  # Stack and compute mean along dim=0\n\ndef calculate_unknown_score(feat_bef, feat_aft, feat_vec, prototypes, lamda=1.0):\n    # 1. Compute multilayer feature distance (s_total)\n    s_total = compute_layer_distances(feat_bef, feat_aft, temperature=0.5)  # Can keep temp fixed or expose as param\n\n    # 2. Compute max prototype similarity (s_prototypes)\n    fv_n = F.normalize(feat_vec, dim=1)  # (B, D)\n    p_n = F.normalize(prototypes, dim=1)  # (C, D)\n    sim = torch.matmul(fv_n, p_n.T)  # (B, C)\n    s_proto, _ = sim.max(dim=1)  # (B,)\n\n    # 3. Final score using lambda\n    score = s_proto - lamda * s_total\n\n    return score  # (B,)\n\n\n\n# =============================\n# Training: CLP\n# =============================\ndef train_CLP(model, loader, epochs, lr, ckpt, alpha_cl=1.0, alpha_rec=1.0):\n    path = os.path.join(CHECKPOINT_DIR, ckpt)\n    opt = torch.optim.Adam(model.parameters(), lr=lr)\n    start = 0\n    if os.path.exists(path):\n        ck = torch.load(path)\n        model.load_state_dict(ck['model'])\n        opt.load_state_dict(ck['opt'])\n        start = ck['ep'] + 1\n    scl = SupConLoss()\n    model.to(DEVICE)\n\n    for ep in range(start, epochs):\n        model.train()\n        total = 0.0\n        for x, y in tqdm(loader, desc=f\"CLP Ep{ep+1}\"):\n            x, y = x.to(DEVICE), y.to(DEVICE)\n            x1 = x + 0.05 * torch.randn_like(x)\n            x2 = x + 0.05 * torch.randn_like(x)\n            f1, p1, r1_loss, r1, _ = model(x1, return_feats=True)\n            f2, p2, r2_loss, r2, _ = model(x2, return_feats=True)\n            l_cl = scl(p1, p2)\n            l_rec = r1_loss.mean() + r2_loss.mean()\n            loss = alpha_cl * l_cl + alpha_rec * l_rec\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n            total += loss.item()\n        torch.save({'model': model.state_dict(), 'opt': opt.state_dict(), 'ep': ep}, path)\n        print(f\"CLP Epoch {ep+1}: {total/len(loader):.4f}\")\n\n\n# =============================\n# Training: PLF\n# =============================\ndef train_PLF(model, loader, epochs, lr, ckpt, num_classes, encoder_pre):\n    path = os.path.join(CHECKPOINT_DIR, ckpt)\n    # unwrap if DataParallel\n    base_model = model.module if isinstance(model, nn.DataParallel) else model\n    embed_dim = base_model.embed_dim\n    proto_tensor = torch.randn(num_classes, embed_dim, device=DEVICE)\n    proto = nn.Parameter(proto_tensor, requires_grad=True)\n    opt = torch.optim.Adam(list(model.parameters()) + [proto], lr=lr)\n    start = 0\n    if os.path.exists(path):\n        ck = torch.load(path)\n        model.load_state_dict(ck['model'])\n        proto.data = ck['proto']\n        opt.load_state_dict(ck['opt'])\n        start = ck['ep'] + 1\n    ploss = ProtoLoss()\n    model.to(DEVICE)\n\n    for ep in range(start, epochs):\n        model.train()\n        total = 0.0\n        for x, y in tqdm(loader, desc=f\"PLF Ep{ep+1}\"):\n            x, y = x.to(DEVICE), y.to(DEVICE)\n            feats, _, _, _, _ = model(x, return_feats=True)\n            loss = ploss(feats, proto, y)\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n            total += loss.item()\n        torch.save({'model': model.state_dict(), 'proto': proto.data, 'opt': opt.state_dict(), 'ep': ep}, path)\n        print(f\"PLF Epoch {ep+1}: {total/len(loader):.4f}\")\n    return proto\n\n# =============================\n# Inference\n# =============================\n\n@torch.no_grad()\ndef inference(enc_pre, model, proto, loader, threshold, lamda=1.0):\n    enc_pre.eval()\n    model.eval()\n    preds = []\n\n    for x, _ in tqdm(loader, desc=\"Infer\"):\n        x = x.to(DEVICE)\n        enc_pre = enc_pre.to(DEVICE)\n        model = model.to(DEVICE)\n\n        _, _, _, _, bef_feats = enc_pre(x, return_feats=True)\n        feat, _, _, _, aft_feats = model(x, return_feats=True)\n\n        # 1. Calculate unknown score\n        scores = calculate_unknown_score(bef_feats, aft_feats, feat, proto, lamda=lamda)  # shape: (B,)\n\n        # 2. Predict the most similar known class\n        idx = torch.argmax(torch.matmul(F.normalize(feat, dim=1), F.normalize(proto, dim=1).T), dim=1)  # (B,)\n\n        # 3. Threshold to filter unknowns\n        known = scores > threshold\n        pred = idx.clone()\n        pred[~known] = -1  # mark unknowns\n\n        preds.append(pred.cpu())\n\n    return torch.cat(preds, dim=0)\n\n\n# =============================\n# Submission\n# =============================\ndef generate_submission(root, preds, db_ds):\n    sub = pd.read_csv(f\"{root}/sample_submission.csv\")\n    meta = pd.read_csv(f\"{root}/metadata.csv\")\n    q = meta[meta['path'].str.contains('/query/')].reset_index(drop=True)\n    q['pred_idx'] = preds.numpy()\n\n    # Step 1: Print raw predicted indices\n    print(\"\\nRaw predicted indices (pred_idx):\")\n    print(q[['image_id', 'pred_idx']].head(10))\n\n    # Step 2: Index to identity mapping\n    idx2id = {v: k for k, v in db_ds.id2idx.items()}\n    q['prediction'] = q['pred_idx'].apply(lambda i: 'new_individual' if i < 0 else idx2id.get(int(i), f\"unknown_{i}\"))\n\n    # Step 3: Print mapped predictions\n    print(\"\\nMapped predictions (after idx2id):\")\n    print(q[['image_id', 'prediction']].head(10))\n\n    out = sub[['image_id']].merge(q[['image_id', 'prediction']], on='image_id')\n\n    import time\n    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n    save_path = f'/kaggle/working/submission_{timestamp}.csv'\n    out.to_csv(save_path, index=False)\n\n    return save_path\n \n# =============================\n# Main Workflow\n# =============================\ndef main():\n    root = '/kaggle/input/animal-clef-2025'\n    tf = transforms.Compose([\n        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n        transforms.ToTensor()\n    ])\n    #idx2id = {v: k for k, v in db_ds.id2idx.items()}\n\n    db_ds = AnimalCLEFDataset(root, 'database', transform=tf)\n    db_loader = DataLoader(db_ds, batch_size=8, shuffle=True,\n                           num_workers=NUM_WORKERS, pin_memory=True)\n    query_ds = AnimalCLEFDataset(root, 'query', transform=tf)\n    q_loader = DataLoader(query_ds, batch_size=8, shuffle=False,\n                           num_workers=NUM_WORKERS, pin_memory=True)\n\n    clp_model = MAEFramework()\n    plf_model = MAEFramework()\n\n    num_gpus = torch.cuda.device_count()\n    if num_gpus >= 2:\n        device_ids = [0, 1]\n    elif num_gpus == 1:\n        device_ids = [0]\n    else:\n        device_ids = None\n\n    clp_model = nn.DataParallel(clp_model, device_ids=device_ids).to(DEVICE)\n    plf_model = nn.DataParallel(plf_model, device_ids=device_ids).to(DEVICE)\n\n    train_CLP(clp_model, db_loader, EPOCHS_CLP, LR, 'clp.pth', alpha_cl=1.0, alpha_rec=1.0)\n    prototype = train_PLF(plf_model, db_loader, EPOCHS_PLF, LR,\n                          'plf.pth', len(db_ds.id2idx), clp_model)\n    print(\"Prototype shape:\", prototype.shape)\n    print(\"Number of unique classes:\", len(db_ds.id2idx))\n\n    # Plot the score distribution before estimating threshold\n    #plot_score_distribution(clp_model, plf_model, prototype, db_loader)\n\n    # Estimate threshold from database distribution\n    dists = []\n    with torch.no_grad():\n        for x, _ in DataLoader(db_ds, batch_size=8, shuffle=False,\n                               num_workers=NUM_WORKERS, pin_memory=True):\n            x = x.to(DEVICE)\n            feat, _, _, _, aft_feats = plf_model(x, return_feats=True)\n            _, _, _, _, bef_feats = clp_model(x, return_feats=True)\n            scores = calculate_unknown_score(bef_feats, aft_feats, feat, prototype)\n            dists.extend(scores.cpu().tolist())\n\n\n    threshold = torch.quantile(torch.tensor(dists), 0.95)\n\n\n    # Inference on query set\n    with torch.no_grad():\n        lamda = 0.2 # or any value you want to test\n        preds = inference(clp_model, plf_model, prototype, q_loader, threshold, lamda=lamda)\n\n\n    print(\"Saving submissionâ€¦\")\n    generate_submission(root, preds, db_ds)\n\nif __name__ == '__main__':\n    main()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:44:13.918842Z","iopub.execute_input":"2025-05-09T03:44:13.919371Z","iopub.status.idle":"2025-05-09T04:18:59.475615Z","shell.execute_reply.started":"2025-05-09T03:44:13.919346Z","shell.execute_reply":"2025-05-09T04:18:59.474736Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 330M/330M [00:01<00:00, 205MB/s]  \nCLP Ep1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1635/1635 [15:00<00:00,  1.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"CLP Epoch 1: 1.3089\n","output_type":"stream"},{"name":"stderr","text":"PLF Ep1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1635/1635 [08:10<00:00,  3.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"PLF Epoch 1: 6.0188\nPrototype shape: torch.Size([1102, 768])\nNumber of unique classes: 1102\n","output_type":"stream"},{"name":"stderr","text":"Infer: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 267/267 [00:49<00:00,  5.35it/s]","output_type":"stream"},{"name":"stdout","text":"Saving submissionâ€¦\n\nRaw predicted indices (pred_idx):\n   image_id  pred_idx\n0         3        24\n1         5        24\n2        12        24\n3        13        35\n4        18        24\n5        19        24\n6        27        24\n7        33        40\n8        36        24\n9        45        24\n\nMapped predictions (after idx2id):\n   image_id          prediction\n0         3  LynxID2025_lynx_32\n1         5  LynxID2025_lynx_32\n2        12  LynxID2025_lynx_32\n3        13  LynxID2025_lynx_43\n4        18  LynxID2025_lynx_32\n5        19  LynxID2025_lynx_32\n6        27  LynxID2025_lynx_32\n7        33  LynxID2025_lynx_49\n8        36  LynxID2025_lynx_32\n9        45  LynxID2025_lynx_32\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}