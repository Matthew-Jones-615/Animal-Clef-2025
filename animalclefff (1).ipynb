{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":91451,"databundleVersionId":11223220,"sourceType":"competition"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport pandas as pd\nfrom torchvision.models.vision_transformer import vit_b_16, ViT_B_16_Weights\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\n# =============================\n# Configurations\n# =============================\nBATCH_SIZE = 32\nNUM_WORKERS = 4\nIMAGE_SIZE = 224\nEPOCHS_CLP = 20\nEPOCHS_PLF = 20\nLR = 1e-4\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nCHECKPOINT_DIR = './checkpoints'\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\n\n# reproducibility\ntorch.manual_seed(42)\n\n# =============================\n# Dataset\n# =============================\nclass AnimalCLEFDataset(Dataset):\n    def __init__(self, root, split=\"database\", transform=None):\n        self.root = root.rstrip('/')\n        meta = pd.read_csv(f\"{self.root}/metadata.csv\")\n        sel = meta[meta['path'].str.contains(f\"/{split}/\")].reset_index(drop=True)\n        if sel.empty:\n            raise ValueError(f\"No entries for split '{split}'\")\n        self.paths = sel['path'].tolist()\n        if split == 'database':\n            ids = sel['image_id'].astype(str)\n            self.id2idx = {iid: i for i, iid in enumerate(sorted(ids.unique()))}\n            self.labels = ids.map(self.id2idx).tolist()\n            num_classes = len(self.id2idx)\n            assert all(0 <= label < num_classes for label in self.labels), \"Invalid labels found\"\n        else:\n            self.labels = [-1] * len(sel)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.paths)\n\n    def __getitem__(self, i):\n        img = Image.open(f\"{self.root}/{self.paths[i]}\").convert('RGB')\n        if self.transform:\n            img = self.transform(img)\n        return img, self.labels[i]\n\n# =============================\n# MAE Encoder + Projection Head + Decoder\n# =============================\nclass MAEFramework(nn.Module):\n    def __init__(self,\n                 embed_dim: int = 768,\n                 proj_dim: int = 256,\n                 decoder_dim: int = 256,\n                 layer_indices: list[int] = [3, 6, 9]):\n        super().__init__()\n        self.embed_dim = embed_dim\n        # 1) Backbone ViT\n        self.encoder = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n        self.encoder.head = nn.Identity()\n        self.layer_indices = set(layer_indices)\n\n        # 3) Projection head\n        self.proj_head = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(embed_dim, proj_dim),\n        )\n        # 4) Decoder\n        self.decoder = nn.Sequential(\n            nn.Linear(embed_dim, decoder_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(decoder_dim, 3 * IMAGE_SIZE * IMAGE_SIZE),\n        )\n\n    def forward(self, x: torch.Tensor, return_feats: bool = False):\n        B = x.size(0)\n        # patch embed\n        x_p = self.encoder.conv_proj(x)\n        x_p = x_p.flatten(2).transpose(1, 2)\n        cls_tok = self.encoder.class_token.expand(B, -1, -1)\n        tokens = torch.cat([cls_tok, x_p], dim=1)\n        tokens = tokens + self.encoder.encoder.pos_embedding\n\n        feats = []\n        for idx, block in enumerate(self.encoder.encoder.layers):\n            tokens = block(tokens)\n            if idx in self.layer_indices:\n                feats.append(tokens.clone())\n\n        cls_feat = self.encoder.encoder.ln(tokens[:, 0])\n        proj = self.proj_head(cls_feat)\n        rec = self.decoder(cls_feat).view(B, 3, IMAGE_SIZE, IMAGE_SIZE)\n        rec_loss = F.mse_loss(rec, x, reduction='none').mean([1, 2, 3])\n\n        if return_feats:\n            return cls_feat, proj, rec_loss, rec, feats\n        return proj, rec_loss, rec\n\n# =============================\n# Losses\n# =============================\nclass SupConLoss(nn.Module):\n    def __init__(self, temperature=0.5):\n        super().__init__()\n        self.temp = temperature\n\n    def forward(self, p1, p2, labels=None):\n        # NT-Xent instance-level contrastive loss\n        z = torch.cat([p1, p2], dim=0)\n        z = F.normalize(z, dim=1)\n        N = p1.size(0)\n        sim = torch.matmul(z, z.T) / self.temp\n        mask = torch.eye(2*N, device=sim.device).bool()\n        sim.masked_fill_(mask, -9e15)\n        idx = torch.arange(N, device=sim.device)\n        targets = torch.cat([idx + N, idx])\n        return F.cross_entropy(sim, targets)\n\nclass ProtoLoss(nn.Module):\n    def forward(self, feats, prototypes, labels):\n        dist = torch.cdist(feats, prototypes)\n        return F.cross_entropy(-dist, labels)\n\n# =============================\n# Utilities\n# =============================\ndef compute_layer_distances(bef_feats, aft_feats, temperature=0.5):\n    total = []  # Initialize as a list to store distances per layer\n    for b, a in zip(bef_feats, aft_feats):\n        b_f, a_f = b.flatten(1), a.flatten(1)\n        eu = F.pairwise_distance(b_f, a_f)\n        cos = 1 - F.cosine_similarity(b_f, a_f, dim=1)\n        score = eu + temperature * cos  # this score is already (B,)\n        total.append(score)  # Add the current batch's score to the list\n    return torch.stack(total, dim=0).mean(dim=0)  # Stack and compute mean along dim=0\n\ndef calculate_unknown_score(feat_bef, feat_aft, feat_vec, prototypes, temperature=0.5, eps=1e-8):\n    # 1. Compute layer distance\n    ld = compute_layer_distances(feat_bef, feat_aft, temperature)  # shape: (B,)\n\n    # 2. Compute cosine similarity to prototypes\n    fv_n = F.normalize(feat_vec, dim=1)  # (B, D)\n    p_n = F.normalize(prototypes, dim=1)  # (C, D)\n    sim = torch.matmul(fv_n, p_n.T)  # (B, C)\n\n    # 3. Use 1 - max similarity\n    max_sim, _ = sim.max(dim=1)  # (B,)\n    proto_dist = 1 - max_sim  # (B,)\n\n    # 4. Combine scores (no unsqueeze needed)\n    score = ld + temperature * proto_dist  # (B,)\n\n    return score  # shape: (B,)\n\n\n# =============================\n# Training: CLP\n# =============================\ndef train_CLP(model, loader, epochs, lr, ckpt):\n    path = os.path.join(CHECKPOINT_DIR, ckpt)\n    opt = torch.optim.Adam(model.parameters(), lr=lr)\n    start = 0\n    if os.path.exists(path):\n        ck = torch.load(path)\n        model.load_state_dict(ck['model'])\n        opt.load_state_dict(ck['opt'])\n        start = ck['ep'] + 1\n    scl = SupConLoss()\n    model.to(DEVICE)\n\n    for ep in range(start, epochs):\n        model.train()\n        total = 0.0\n        for x, y in tqdm(loader, desc=f\"CLP Ep{ep+1}\"):\n            x, y = x.to(DEVICE), y.to(DEVICE)\n            x1 = x + 0.05 * torch.randn_like(x)\n            x2 = x + 0.05 * torch.randn_like(x)\n            f1, p1, r1_loss, r1, _ = model(x1, return_feats=True)\n            f2, p2, r2_loss, r2, _ = model(x2, return_feats=True)\n            l_cl = scl(p1, p2)\n            l_rec = r1_loss.mean() + r2_loss.mean()\n            loss = l_cl + l_rec\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n            total += loss.item()\n        torch.save({'model': model.state_dict(), 'opt': opt.state_dict(), 'ep': ep}, path)\n        print(f\"CLP Epoch {ep+1}: {total/len(loader):.4f}\")\n\n# =============================\n# Training: PLF\n# =============================\ndef train_PLF(model, loader, epochs, lr, ckpt, num_classes, encoder_pre):\n    path = os.path.join(CHECKPOINT_DIR, ckpt)\n    # unwrap if DataParallel\n    base_model = model.module if isinstance(model, nn.DataParallel) else model\n    embed_dim = base_model.embed_dim\n    proto_tensor = torch.randn(num_classes, embed_dim, device=DEVICE)\n    proto = nn.Parameter(proto_tensor, requires_grad=True)\n    opt = torch.optim.Adam(list(model.parameters()) + [proto], lr=lr)\n    start = 0\n    if os.path.exists(path):\n        ck = torch.load(path)\n        model.load_state_dict(ck['model'])\n        proto.data = ck['proto']\n        opt.load_state_dict(ck['opt'])\n        start = ck['ep'] + 1\n    ploss = ProtoLoss()\n    model.to(DEVICE)\n\n    for ep in range(start, epochs):\n        model.train()\n        total = 0.0\n        for x, y in tqdm(loader, desc=f\"PLF Ep{ep+1}\"):\n            x, y = x.to(DEVICE), y.to(DEVICE)\n            feats, _, _, _, _ = model(x, return_feats=True)\n            loss = ploss(feats, proto, y)\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n            total += loss.item()\n        torch.save({'model': model.state_dict(), 'proto': proto.data, 'opt': opt.state_dict(), 'ep': ep}, path)\n        print(f\"PLF Epoch {ep+1}: {total/len(loader):.4f}\")\n    return proto\n\n# =============================\n# Inference\n# =============================\n\n@torch.no_grad()\ndef inference(enc_pre, model, proto, loader, threshold):\n    enc_pre.eval()\n    model.eval()\n    preds = []\n\n    for x, _ in tqdm(loader, desc=\"Infer\"):\n        x = x.to(DEVICE)\n        enc_pre = enc_pre.to(DEVICE)\n        model = model.to(DEVICE)\n\n        _, _, _, _, bef_feats = enc_pre(x, return_feats=True)\n        feat, _, _, _, aft_feats = model(x, return_feats=True)\n\n        scores = calculate_unknown_score(bef_feats, aft_feats, feat, proto)  # shape: (B,)\n        min_s, idx = scores.min(dim=1)  # shape: (B,)\n\n        known = min_s < threshold\n        pred = idx.clone()\n        pred[~known] = -1  # mark unknowns\n\n        preds.append(pred.cpu())\n\n    return torch.cat(preds, dim=0)\n\n# =============================\n# Submission\n# =============================\ndef generate_submission(root, preds, db_ds):\n    sub = pd.read_csv(f\"{root}/sample_submission.csv\")\n    meta = pd.read_csv(f\"{root}/metadata.csv\")\n    q = meta[meta['path'].str.contains('/query/')].reset_index(drop=True)\n    q['pred_idx'] = preds.numpy()\n\n    idx2id = {v: k for k, v in db_ds.id2idx.items()}\n    q['prediction'] = q['pred_idx'].apply(lambda i: 'new_individual' if i < 0 else idx2id[int(i)])\n    out = sub[['image_id']].merge(q[['image_id', 'prediction']], on='image_id')\n\n    # Timestamped filename\n    import time\n    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n    save_path = f'/kaggle/working/submission_{timestamp}.csv'\n    out.to_csv(save_path, index=False)\n\n    return save_path\n\ndef plot_score_distribution(enc_pre, model, proto, loader):\n    enc_pre.eval()\n    model.eval()\n    all_scores = []\n\n    for x, _ in tqdm(loader, desc=\"Scoring for Plot\"):\n        x = x.to(DEVICE)\n        enc_pre = enc_pre.to(DEVICE)\n        model = model.to(DEVICE)\n\n        _, _, _, _, bef_feats = enc_pre(x, return_feats=True)\n        feat, _, _, _, aft_feats = model(x, return_feats=True)\n\n        scores = calculate_unknown_score(bef_feats, aft_feats, feat, proto)\n        min_s, _ = scores.min(dim=1)\n        all_scores.append(min_s.cpu())\n\n    all_scores = torch.cat(all_scores).numpy()\n\n    plt.figure(figsize=(8, 5))\n    plt.hist(all_scores, bins=50, alpha=0.75, color='steelblue')\n    plt.xlabel(\"Unknown Score\")\n    plt.ylabel(\"Count\")\n    plt.title(\"Score Distribution — Use to Tune Threshold\")\n    plt.grid(True)\n    plt.show()\n\n# =============================\n# Main Workflow\n# =============================\ndef main():\n    root = '/kaggle/input/animal-clef-2025'\n    tf = transforms.Compose([\n        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n        transforms.ToTensor()\n    ])\n    #idx2id = {v: k for k, v in db_ds.id2idx.items()}\n\n    db_ds = AnimalCLEFDataset(root, 'database', transform=tf)\n    db_loader = DataLoader(db_ds, batch_size=8, shuffle=True,\n                           num_workers=NUM_WORKERS, pin_memory=True)\n    query_ds = AnimalCLEFDataset(root, 'query', transform=tf)\n    q_loader = DataLoader(query_ds, batch_size=8, shuffle=False,\n                           num_workers=NUM_WORKERS, pin_memory=True)\n\n    clp_model = MAEFramework()\n    plf_model = MAEFramework()\n\n    num_gpus = torch.cuda.device_count()\n    if num_gpus >= 2:\n        device_ids = [0, 1]\n    elif num_gpus == 1:\n        device_ids = [0]\n    else:\n        device_ids = None\n\n    clp_model = nn.DataParallel(clp_model, device_ids=device_ids).to(DEVICE)\n    plf_model = nn.DataParallel(plf_model, device_ids=device_ids).to(DEVICE)\n\n    train_CLP(clp_model, db_loader, EPOCHS_CLP, LR, 'clp.pth')\n    prototype = train_PLF(plf_model, db_loader, EPOCHS_PLF, LR,\n                          'plf.pth', len(db_ds.id2idx), clp_model)\n\n    # Plot the score distribution before estimating threshold\n    #plot_score_distribution(clp_model, plf_model, prototype, db_loader)\n\n    # Estimate threshold from database distribution\n    dists = []\n    with torch.no_grad():\n        for x, _ in DataLoader(db_ds, batch_size=8, shuffle=False,\n                               num_workers=NUM_WORKERS, pin_memory=True):\n            x = x.to(DEVICE)\n            feat, _, _, _, aft_feats = plf_model(x, return_feats=True)\n            _, _, _, _, bef_feats = clp_model(x, return_feats=True)\n            scores = calculate_unknown_score(bef_feats, aft_feats, feat, prototype)\n            dists.extend(scores.min(dim=0)[0].cpu().tolist())\n\n    threshold = torch.quantile(torch.cat(dists), 0.95)\n\n    # Inference on query set\n    with torch.no_grad():\n        preds = inference(clp_model, plf_model, prototype, q_loader, threshold)\n\n    print(\"Saving submission…\")\n    generate_submission(root, preds, db_ds)\n\nif __name__ == '__main__':\n    main()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-08T20:22:32.289674Z","iopub.execute_input":"2025-05-08T20:22:32.290171Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n100%|██████████| 330M/330M [00:01<00:00, 176MB/s]  \nCLP Ep1: 100%|██████████| 1635/1635 [09:21<00:00,  2.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"CLP Epoch 1: 1.3126\n","output_type":"stream"},{"name":"stderr","text":"CLP Ep2: 100%|██████████| 1635/1635 [09:20<00:00,  2.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"CLP Epoch 2: 1.2318\n","output_type":"stream"},{"name":"stderr","text":"CLP Ep3: 100%|██████████| 1635/1635 [09:20<00:00,  2.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"CLP Epoch 3: 1.2261\n","output_type":"stream"},{"name":"stderr","text":"CLP Ep4: 100%|██████████| 1635/1635 [09:20<00:00,  2.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"CLP Epoch 4: 1.2161\n","output_type":"stream"},{"name":"stderr","text":"CLP Ep5: 100%|██████████| 1635/1635 [09:19<00:00,  2.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"CLP Epoch 5: 1.2157\n","output_type":"stream"},{"name":"stderr","text":"CLP Ep6: 100%|██████████| 1635/1635 [09:20<00:00,  2.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"CLP Epoch 6: 1.2030\n","output_type":"stream"},{"name":"stderr","text":"CLP Ep7: 100%|██████████| 1635/1635 [09:19<00:00,  2.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"CLP Epoch 7: 1.2148\n","output_type":"stream"},{"name":"stderr","text":"CLP Ep8: 100%|██████████| 1635/1635 [09:19<00:00,  2.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"CLP Epoch 8: 1.2180\n","output_type":"stream"},{"name":"stderr","text":"CLP Ep9: 100%|██████████| 1635/1635 [09:19<00:00,  2.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"CLP Epoch 9: 1.2193\n","output_type":"stream"},{"name":"stderr","text":"CLP Ep10: 100%|██████████| 1635/1635 [09:19<00:00,  2.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"CLP Epoch 10: 1.2000\n","output_type":"stream"},{"name":"stderr","text":"CLP Ep11: 100%|██████████| 1635/1635 [09:19<00:00,  2.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"CLP Epoch 11: 1.2184\n","output_type":"stream"},{"name":"stderr","text":"CLP Ep12: 100%|██████████| 1635/1635 [09:19<00:00,  2.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"CLP Epoch 12: 1.2023\n","output_type":"stream"},{"name":"stderr","text":"CLP Ep13: 100%|██████████| 1635/1635 [09:19<00:00,  2.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"CLP Epoch 13: 1.1855\n","output_type":"stream"},{"name":"stderr","text":"CLP Ep14: 100%|██████████| 1635/1635 [09:19<00:00,  2.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"CLP Epoch 14: 1.1930\n","output_type":"stream"},{"name":"stderr","text":"CLP Ep15: 100%|██████████| 1635/1635 [09:19<00:00,  2.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"CLP Epoch 15: 1.1908\n","output_type":"stream"},{"name":"stderr","text":"CLP Ep16: 100%|██████████| 1635/1635 [09:20<00:00,  2.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"CLP Epoch 16: 1.1880\n","output_type":"stream"},{"name":"stderr","text":"CLP Ep17: 100%|██████████| 1635/1635 [09:20<00:00,  2.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"CLP Epoch 17: 1.1881\n","output_type":"stream"},{"name":"stderr","text":"CLP Ep18: 100%|██████████| 1635/1635 [09:20<00:00,  2.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"CLP Epoch 18: 1.1830\n","output_type":"stream"},{"name":"stderr","text":"CLP Ep19: 100%|██████████| 1635/1635 [09:20<00:00,  2.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"CLP Epoch 19: 1.1669\n","output_type":"stream"},{"name":"stderr","text":"CLP Ep20:  91%|█████████ | 1490/1635 [08:30<00:49,  2.92it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}