{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torchvision.models.vision_transformer import vit_b_16, ViT_B_16_Weights\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from animal_dataset import AnimalCLEFDataset  # now importable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10e51d990>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================\n",
    "# Configurations\n",
    "# =============================\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 4\n",
    "IMAGE_SIZE = 224\n",
    "EPOCHS_CLP = 1\n",
    "EPOCHS_PLF = 1\n",
    "LR = 1e-4\n",
    "#DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "CHECKPOINT_DIR = './checkpoints'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "root = '../animal-clef-2025_data'\n",
    "\n",
    "# reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Set device, Mac, GPU, or CPU\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# =============================\n",
    "# Dataset\n",
    "# =============================\n",
    "class AnimalCLEFDataset(Dataset):\n",
    "    def __init__(self, root, split=\"database\", transform=None):\n",
    "        self.root = root.rstrip('/')\n",
    "        meta = pd.read_csv(f\"{self.root}/metadata.csv\")\n",
    "        sel = meta[meta['path'].str.contains(f\"/{split}/\")].reset_index(drop=True)\n",
    "        if sel.empty:\n",
    "            raise ValueError(f\"No entries for split '{split}'\")\n",
    "\n",
    "        self.paths = sel['path'].tolist()\n",
    "        self.image_ids = sel['image_id'].tolist()\n",
    "\n",
    "        if split == 'database':\n",
    "            #  Use individual identity,  \n",
    "            ids = sel['identity'].astype(str)\n",
    "\n",
    "            #  Build mapping from identity string → label index\n",
    "            self.id2idx = {iid: i for i, iid in enumerate(sorted(ids.unique()))}\n",
    "\n",
    "            #  Map each sample's identity to its label\n",
    "            self.labels = ids.map(self.id2idx).tolist()\n",
    "\n",
    "            # Safety check\n",
    "            num_classes = len(self.id2idx)\n",
    "            assert all(0 <= label < num_classes for label in self.labels), \"Invalid labels found\"\n",
    "        else:\n",
    "            self.labels = [-1] * len(sel)\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img = Image.open(f\"{self.root}/{self.paths[i]}\").convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, self.labels[i]\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# MAE Encoder + Projection Head + Decoder\n",
    "# =============================\n",
    "class MAEFramework(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int = 768,\n",
    "                 proj_dim: int = 256,\n",
    "                 decoder_dim: int = 256,\n",
    "                 layer_indices: list[int] = [3, 6, 9]):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        # 1) Backbone ViT\n",
    "        self.encoder = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "        self.encoder.head = nn.Identity()\n",
    "        self.layer_indices = set(layer_indices)\n",
    "\n",
    "        # 3) Projection head\n",
    "        self.proj_head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(embed_dim, proj_dim),\n",
    "        )\n",
    "        # 4) Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(embed_dim, decoder_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(decoder_dim, 3 * IMAGE_SIZE * IMAGE_SIZE),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, return_feats: bool = False):\n",
    "        B = x.size(0)\n",
    "        # patch embed\n",
    "        x_p = self.encoder.conv_proj(x)\n",
    "        x_p = x_p.flatten(2).transpose(1, 2)\n",
    "        cls_tok = self.encoder.class_token.expand(B, -1, -1)\n",
    "        tokens = torch.cat([cls_tok, x_p], dim=1)\n",
    "        tokens = tokens + self.encoder.encoder.pos_embedding\n",
    "\n",
    "        feats = []\n",
    "        for idx, block in enumerate(self.encoder.encoder.layers):\n",
    "            tokens = block(tokens)\n",
    "            if idx in self.layer_indices:\n",
    "                feats.append(tokens.clone())\n",
    "\n",
    "        cls_feat = self.encoder.encoder.ln(tokens[:, 0])\n",
    "        proj = self.proj_head(cls_feat)\n",
    "        rec = self.decoder(cls_feat).view(B, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "        rec_loss = F.mse_loss(rec, x, reduction='none').mean([1, 2, 3])\n",
    "\n",
    "        if return_feats:\n",
    "            return cls_feat, proj, rec_loss, rec, feats\n",
    "        return proj, rec_loss, rec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Losses\n",
    "# =============================\n",
    "class SupConLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.5):\n",
    "        super().__init__()\n",
    "        self.temp = temperature\n",
    "\n",
    "    def forward(self, p1, p2, labels=None):\n",
    "        # NT-Xent instance-level contrastive loss\n",
    "        z = torch.cat([p1, p2], dim=0)\n",
    "        z = F.normalize(z, dim=1)\n",
    "        N = p1.size(0)\n",
    "        sim = torch.matmul(z, z.T) / self.temp\n",
    "        mask = torch.eye(2*N, device=sim.device).bool()\n",
    "        sim.masked_fill_(mask, -9e15)\n",
    "        idx = torch.arange(N, device=sim.device)\n",
    "        targets = torch.cat([idx + N, idx])\n",
    "        return F.cross_entropy(sim, targets)\n",
    "\n",
    "class ProtoLoss(nn.Module):\n",
    "    def forward(self, feats, prototypes, labels):\n",
    "        dist = torch.cdist(feats, prototypes)\n",
    "        return F.cross_entropy(-dist, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Utilities\n",
    "# =============================\n",
    "def compute_layer_distances(bef_feats, aft_feats, temperature=0.5):\n",
    "    total = []  # Initialize as a list to store distances per layer\n",
    "    for b, a in zip(bef_feats, aft_feats):\n",
    "        b_f, a_f = b.flatten(1), a.flatten(1)\n",
    "        eu = F.pairwise_distance(b_f, a_f)\n",
    "        cos = 1 - F.cosine_similarity(b_f, a_f, dim=1)\n",
    "        score = eu + temperature * cos  # this score is already (B,)\n",
    "        total.append(score)  # Add the current batch's score to the list\n",
    "    return torch.stack(total, dim=0).mean(dim=0)  # Stack and compute mean along dim=0\n",
    "\n",
    "def calculate_unknown_score(feat_bef, feat_aft, feat_vec, prototypes, lamda=1.0):\n",
    "    # 1. Compute multilayer feature distance (s_total)\n",
    "    s_total = compute_layer_distances(feat_bef, feat_aft, temperature=0.5)  # Can keep temp fixed or expose as param\n",
    "\n",
    "    # 2. Compute max prototype similarity (s_prototypes)\n",
    "    fv_n = F.normalize(feat_vec, dim=1)  # (B, D)\n",
    "    p_n = F.normalize(prototypes, dim=1)  # (C, D)\n",
    "    sim = torch.matmul(fv_n, p_n.T)  # (B, C)\n",
    "    s_proto, _ = sim.max(dim=1)  # (B,)\n",
    "\n",
    "    # 3. Final score using lambda\n",
    "    score = s_proto - lamda * s_total\n",
    "\n",
    "    return score  # (B,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_CLP(model, loader, epochs, lr, ckpt, alpha_cl=1.0, alpha_rec=1.0):\n",
    "    path = os.path.join(CHECKPOINT_DIR, ckpt)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    start = 0\n",
    "\n",
    "    # --- load checkpoint if it exists, stripping \"module.\" if necessary\n",
    "    if os.path.exists(path):\n",
    "        ck = torch.load(path, map_location=DEVICE)\n",
    "        raw_sd = ck['model']\n",
    "        # strip DataParallel \"module.\" prefix\n",
    "        sd = {k.replace(\"module.\", \"\"): v for k, v in raw_sd.items()}\n",
    "        model.load_state_dict(sd)\n",
    "        opt.load_state_dict(ck['opt'])\n",
    "        start = ck['ep'] + 1\n",
    "    # ---------------------------------------------------------------\n",
    "\n",
    "    scl = SupConLoss()\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    for ep in range(start, epochs):\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "        for x, y in tqdm(loader, desc=f\"CLP Ep{ep+1}\"):\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            x1 = x + 0.05 * torch.randn_like(x)\n",
    "            x2 = x + 0.05 * torch.randn_like(x)\n",
    "            f1, p1, r1_loss, r1, _ = model(x1, return_feats=True)\n",
    "            f2, p2, r2_loss, r2, _ = model(x2, return_feats=True)\n",
    "            l_cl  = scl(p1, p2)\n",
    "            l_rec = r1_loss.mean() + r2_loss.mean()\n",
    "            loss  = alpha_cl * l_cl + alpha_rec * l_rec\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total += loss.item()\n",
    "\n",
    "        torch.save({\n",
    "            'model': model.state_dict() if not isinstance(model, nn.DataParallel)\n",
    "                              else model.module.state_dict(),\n",
    "            'opt':   opt.state_dict(),\n",
    "            'ep':    ep\n",
    "        }, path)\n",
    "        print(f\"CLP Epoch {ep+1}: {total/len(loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_PLF(model, loader, epochs, lr, ckpt, num_classes, encoder_pre):\n",
    "    path = os.path.join(CHECKPOINT_DIR, ckpt)\n",
    "    # unwrap if DataParallel\n",
    "    base_model = model.module if isinstance(model, nn.DataParallel) else model\n",
    "    embed_dim   = base_model.embed_dim\n",
    "\n",
    "    proto_tensor = torch.randn(num_classes, embed_dim, device=DEVICE)\n",
    "    proto = nn.Parameter(proto_tensor, requires_grad=True)\n",
    "    opt   = torch.optim.Adam(list(model.parameters()) + [proto], lr=lr)\n",
    "    start = 0\n",
    "\n",
    "    # --- load checkpoint if it exists, stripping \"module.\" if necessary\n",
    "    if os.path.exists(path):\n",
    "        ck = torch.load(path, map_location=DEVICE)\n",
    "        raw_sd = ck['model']\n",
    "        sd = {k.replace(\"module.\", \"\"): v for k, v in raw_sd.items()}\n",
    "        model.load_state_dict(sd)\n",
    "        proto.data = ck['proto']\n",
    "        opt.load_state_dict(ck['opt'])\n",
    "        start = ck['ep'] + 1\n",
    "    # ---------------------------------------------------------------\n",
    "\n",
    "    ploss = ProtoLoss()\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    for ep in range(start, epochs):\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "        for x, y in tqdm(loader, desc=f\"PLF Ep{ep+1}\"):\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            feats, _, _, _, _ = model(x, return_feats=True)\n",
    "            loss = ploss(feats, proto, y)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total += loss.item()\n",
    "\n",
    "        torch.save({\n",
    "            'model': model.state_dict() if not isinstance(model, nn.DataParallel)\n",
    "                              else model.module.state_dict(),\n",
    "            'proto': proto.data,\n",
    "            'opt':   opt.state_dict(),\n",
    "            'ep':    ep\n",
    "        }, path)\n",
    "        print(f\"PLF Epoch {ep+1}: {total/len(loader):.4f}\")\n",
    "\n",
    "    return proto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# =============================\n",
    "# Training: CLP\n",
    "# =============================\n",
    "def train_CLP(model, loader, epochs, lr, ckpt, alpha_cl=1.0, alpha_rec=1.0):\n",
    "    path = os.path.join(CHECKPOINT_DIR, ckpt)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    start = 0\n",
    "    if os.path.exists(path):\n",
    "        ck = torch.load(path)\n",
    "        model.load_state_dict(ck['model'])\n",
    "        opt.load_state_dict(ck['opt'])\n",
    "        start = ck['ep'] + 1\n",
    "    scl = SupConLoss()\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    for ep in range(start, epochs):\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "        for x, y in tqdm(loader, desc=f\"CLP Ep{ep+1}\"):\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            x1 = x + 0.05 * torch.randn_like(x)\n",
    "            x2 = x + 0.05 * torch.randn_like(x)\n",
    "            f1, p1, r1_loss, r1, _ = model(x1, return_feats=True)\n",
    "            f2, p2, r2_loss, r2, _ = model(x2, return_feats=True)\n",
    "            l_cl = scl(p1, p2)\n",
    "            l_rec = r1_loss.mean() + r2_loss.mean()\n",
    "            loss = alpha_cl * l_cl + alpha_rec * l_rec\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total += loss.item()\n",
    "        torch.save({'model': model.state_dict(), 'opt': opt.state_dict(), 'ep': ep}, path)\n",
    "        print(f\"CLP Epoch {ep+1}: {total/len(loader):.4f}\")\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Training: PLF\n",
    "# =============================\n",
    "def train_PLF(model, loader, epochs, lr, ckpt, num_classes, encoder_pre):\n",
    "    path = os.path.join(CHECKPOINT_DIR, ckpt)\n",
    "    # unwrap if DataParallel\n",
    "    base_model = model.module if isinstance(model, nn.DataParallel) else model\n",
    "    embed_dim = base_model.embed_dim\n",
    "    proto_tensor = torch.randn(num_classes, embed_dim, device=DEVICE)\n",
    "    proto = nn.Parameter(proto_tensor, requires_grad=True)\n",
    "    opt = torch.optim.Adam(list(model.parameters()) + [proto], lr=lr)\n",
    "    start = 0\n",
    "    if os.path.exists(path):\n",
    "        ck = torch.load(path)\n",
    "        model.load_state_dict(ck['model'])\n",
    "        proto.data = ck['proto']\n",
    "        opt.load_state_dict(ck['opt'])\n",
    "        start = ck['ep'] + 1\n",
    "    ploss = ProtoLoss()\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    for ep in range(start, epochs):\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "        for x, y in tqdm(loader, desc=f\"PLF Ep{ep+1}\"):\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            feats, _, _, _, _ = model(x, return_feats=True)\n",
    "            loss = ploss(feats, proto, y)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total += loss.item()\n",
    "        torch.save({'model': model.state_dict(), 'proto': proto.data, 'opt': opt.state_dict(), 'ep': ep}, path)\n",
    "        print(f\"PLF Epoch {ep+1}: {total/len(loader):.4f}\")\n",
    "    return proto\n",
    "\"\"\";\n",
    "    \n",
    "# =============================\n",
    "# Inference\n",
    "# =============================\n",
    "\n",
    "@torch.no_grad()\n",
    "def inference(enc_pre, model, proto, loader, threshold, lamda=1.0):\n",
    "    enc_pre.eval()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "\n",
    "    for x, _ in tqdm(loader, desc=\"Infer\"):\n",
    "        x = x.to(DEVICE)\n",
    "        enc_pre = enc_pre.to(DEVICE)\n",
    "        model = model.to(DEVICE)\n",
    "\n",
    "        _, _, _, _, bef_feats = enc_pre(x, return_feats=True)\n",
    "        feat, _, _, _, aft_feats = model(x, return_feats=True)\n",
    "\n",
    "        # 1. Calculate unknown score\n",
    "        scores = calculate_unknown_score(bef_feats, aft_feats, feat, proto, lamda=lamda)  # shape: (B,)\n",
    "\n",
    "        # 2. Predict the most similar known class\n",
    "        idx = torch.argmax(torch.matmul(F.normalize(feat, dim=1), F.normalize(proto, dim=1).T), dim=1)  # (B,)\n",
    "\n",
    "        # 3. Threshold to filter unknowns\n",
    "        known = scores > threshold\n",
    "        pred = idx.clone()\n",
    "        pred[~known] = -1  # mark unknowns\n",
    "\n",
    "        preds.append(pred.cpu())\n",
    "\n",
    "    return torch.cat(preds, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Submission\n",
    "# =============================\n",
    "def generate_submission(root, preds, db_ds):\n",
    "    sub = pd.read_csv(f\"{root}/sample_submission.csv\")\n",
    "    meta = pd.read_csv(f\"{root}/metadata.csv\")\n",
    "    q = meta[meta['path'].str.contains('/query/')].reset_index(drop=True)\n",
    "    q['pred_idx'] = preds.numpy()\n",
    "\n",
    "    # Step 1: Print raw predicted indices\n",
    "    print(\"\\nRaw predicted indices (pred_idx):\")\n",
    "    print(q[['image_id', 'pred_idx']].head(10))\n",
    "\n",
    "    # Step 2: Index to identity mapping\n",
    "    idx2id = {v: k for k, v in db_ds.id2idx.items()}\n",
    "    q['prediction'] = q['pred_idx'].apply(lambda i: 'new_individual' if i < 0 else idx2id.get(int(i), f\"unknown_{i}\"))\n",
    "\n",
    "    # Step 3: Print mapped predictions\n",
    "    print(\"\\nMapped predictions (after idx2id):\")\n",
    "    print(q[['image_id', 'prediction']].head(10))\n",
    "\n",
    "    out = sub[['image_id']].merge(q[['image_id', 'prediction']], on='image_id')\n",
    "\n",
    "    import time\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    save_path = f'submission_{timestamp}.csv'\n",
    "    out.to_csv(save_path, index=False)\n",
    "\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototype shape: torch.Size([1102, 768])\n",
      "Number of unique classes: 1102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthewjones2/anaconda3/envs/574_animal_clef_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Infer: 100%|██████████| 267/267 [00:52<00:00,  5.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving submission…\n",
      "\n",
      "Raw predicted indices (pred_idx):\n",
      "   image_id  pred_idx\n",
      "0         3        50\n",
      "1         5        24\n",
      "2        12        35\n",
      "3        13        35\n",
      "4        18        35\n",
      "5        19        35\n",
      "6        27        50\n",
      "7        33        35\n",
      "8        36        50\n",
      "9        45        24\n",
      "\n",
      "Mapped predictions (after idx2id):\n",
      "   image_id          prediction\n",
      "0         3  LynxID2025_lynx_62\n",
      "1         5  LynxID2025_lynx_32\n",
      "2        12  LynxID2025_lynx_43\n",
      "3        13  LynxID2025_lynx_43\n",
      "4        18  LynxID2025_lynx_43\n",
      "5        19  LynxID2025_lynx_43\n",
      "6        27  LynxID2025_lynx_62\n",
      "7        33  LynxID2025_lynx_43\n",
      "8        36  LynxID2025_lynx_62\n",
      "9        45  LynxID2025_lynx_32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Main Workflow\n",
    "# =============================\n",
    "def main():\n",
    "    #root = '/kaggle/input/animal-clef-2025'\n",
    "    tf = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    #idx2id = {v: k for k, v in db_ds.id2idx.items()}\n",
    "\n",
    "    db_ds = AnimalCLEFDataset(root, 'database', transform=tf)\n",
    "    \n",
    "    #print(\"Example metadata path[0]:\", db_ds.paths[0])\n",
    "    #print(\"Looking for file at:\", os.path.join(db_ds.root, db_ds.paths[0]))\n",
    "    #print(\"Exists on disk?\", os.path.exists(os.path.join(db_ds.root, db_ds.paths[0])))\n",
    "    \n",
    "    db_loader = DataLoader(db_ds, batch_size=8, shuffle=True,\n",
    "                           num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    query_ds = AnimalCLEFDataset(root, 'query', transform=tf)\n",
    "    q_loader = DataLoader(query_ds, batch_size=8, shuffle=False,\n",
    "                           num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "    clp_model = MAEFramework()\n",
    "    plf_model = MAEFramework()\n",
    "\n",
    "    # pick your device\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        DEVICE = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        DEVICE = torch.device(\"cuda\")\n",
    "    else:\n",
    "        DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "    # decide which GPUs to wrap\n",
    "    cuda_gpus = torch.cuda.device_count()\n",
    "    if DEVICE.type == \"cuda\" and cuda_gpus > 1:\n",
    "        device_ids = list(range(cuda_gpus))  # e.g. [0,1,…]\n",
    "        clp_model = nn.DataParallel(clp_model,  device_ids=device_ids).to(DEVICE)\n",
    "        plf_model = nn.DataParallel(plf_model,  device_ids=device_ids).to(DEVICE)\n",
    "    else:\n",
    "        # single‐device: MPS, single CUDA, or CPU\n",
    "        clp_model = clp_model.to(DEVICE)\n",
    "        plf_model = plf_model.to(DEVICE)\n",
    "\n",
    "    # now training and inference will work on either MPS or CUDA\n",
    "    train_CLP(clp_model, db_loader, EPOCHS_CLP, LR,  'clp.pth', alpha_cl=1.0, alpha_rec=1.0)\n",
    "    prototype = train_PLF(plf_model, db_loader, EPOCHS_PLF, LR,\n",
    "                          'plf.pth', len(db_ds.id2idx), clp_model)\n",
    "    \n",
    "    print(\"Prototype shape:\", prototype.shape)\n",
    "    print(\"Number of unique classes:\", len(db_ds.id2idx))\n",
    "\n",
    "    # Plot the score distribution before estimating threshold\n",
    "    #plot_score_distribution(clp_model, plf_model, prototype, db_loader)\n",
    "\n",
    "    # Estimate threshold from database distribution\n",
    "    dists = []\n",
    "    with torch.no_grad():\n",
    "        for x, _ in DataLoader(db_ds, batch_size=8, shuffle=False,\n",
    "                               num_workers=NUM_WORKERS, pin_memory=True):\n",
    "            x = x.to(DEVICE)\n",
    "            feat, _, _, _, aft_feats = plf_model(x, return_feats=True)\n",
    "            _, _, _, _, bef_feats = clp_model(x, return_feats=True)\n",
    "            scores = calculate_unknown_score(bef_feats, aft_feats, feat, prototype)\n",
    "            dists.extend(scores.cpu().tolist())\n",
    "\n",
    "\n",
    "    threshold = torch.quantile(torch.tensor(dists), 0.95)\n",
    "\n",
    "\n",
    "    # Inference on query set\n",
    "    with torch.no_grad():\n",
    "        lamda = 0.2 # or any value you want to test\n",
    "        preds = inference(clp_model, plf_model, prototype, q_loader, threshold, lamda=lamda)\n",
    "\n",
    "\n",
    "    print(\"Saving submission…\")\n",
    "    generate_submission(root, preds, db_ds)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clp_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m preds = inference(\u001b[43mclp_model\u001b[49m, plf_model, prototype, q_loader, threshold, lamda=lamda)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSaving submission…\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m generate_submission(root, preds, db_ds)\n",
      "\u001b[31mNameError\u001b[39m: name 'clp_model' is not defined"
     ]
    }
   ],
   "source": [
    "#preds = inference(clp_model, plf_model, prototype, q_loader, threshold, lamda=lamda)\n",
    "#print(\"Saving submission…\")\n",
    "#generate_submission(root, preds, db_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-09T03:44:13.919371Z",
     "iopub.status.busy": "2025-05-09T03:44:13.918842Z",
     "iopub.status.idle": "2025-05-09T04:18:59.475615Z",
     "shell.execute_reply": "2025-05-09T04:18:59.474736Z",
     "shell.execute_reply.started": "2025-05-09T03:44:13.919346Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n",
      "100%|██████████| 330M/330M [00:01<00:00, 205MB/s]  \n",
      "CLP Ep1: 100%|██████████| 1635/1635 [15:00<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLP Epoch 1: 1.3089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PLF Ep1: 100%|██████████| 1635/1635 [08:10<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLF Epoch 1: 6.0188\n",
      "Prototype shape: torch.Size([1102, 768])\n",
      "Number of unique classes: 1102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer: 100%|██████████| 267/267 [00:49<00:00,  5.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving submission…\n",
      "\n",
      "Raw predicted indices (pred_idx):\n",
      "   image_id  pred_idx\n",
      "0         3        24\n",
      "1         5        24\n",
      "2        12        24\n",
      "3        13        35\n",
      "4        18        24\n",
      "5        19        24\n",
      "6        27        24\n",
      "7        33        40\n",
      "8        36        24\n",
      "9        45        24\n",
      "\n",
      "Mapped predictions (after idx2id):\n",
      "   image_id          prediction\n",
      "0         3  LynxID2025_lynx_32\n",
      "1         5  LynxID2025_lynx_32\n",
      "2        12  LynxID2025_lynx_32\n",
      "3        13  LynxID2025_lynx_43\n",
      "4        18  LynxID2025_lynx_32\n",
      "5        19  LynxID2025_lynx_32\n",
      "6        27  LynxID2025_lynx_32\n",
      "7        33  LynxID2025_lynx_49\n",
      "8        36  LynxID2025_lynx_32\n",
      "9        45  LynxID2025_lynx_32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11223220,
     "sourceId": 91451,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "574_animal_clef_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
